This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-11-29T01:52:01.165Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
public/
  file.svg
  globe.svg
  next.svg
  vercel.svg
  window.svg
src/
  app/
    api/
      delete-memory-log/
        route.ts
      images/
        route.ts
      print/
        route.ts
      socket/
        route.ts
      transcription/
        [sessionId]/
          route.ts
    session/
      [sessionId]/
        page.tsx
      page.tsx
    fonts.ts
    globals.css
    layout.tsx
    page.tsx
  components/
    layout/
      AudioLevelIndicator.tsx
      Footer.tsx
      Header.tsx
      LayoutClient.tsx
      MainContent.tsx
      RecordingControls.tsx
      Sidebar.tsx
    transcription/
      TranscriptionViewer.tsx
    ui/
      button.tsx
      card.tsx
    ImageDisplay.tsx
    RecordingProgress.tsx
    WelcomeOverlay.tsx
  lib/
    constants/
      paths.js
    hooks/
      useAudioRecorder.ts
      useImageNavigation.ts
    services/
      audioSession.js
      generateImageFlux.js
      generateImagePrompt.js
      llmPrompt.js
      memoryProcessor.js
      memoryPrompt.js
      recordingService.js
      transcription.js
    test/
      test-transcription.js
    types/
      audio.ts
      transcription.ts
    utils/
      audio.ts
      env.ts
      session.js
      session.ts
      ui.ts
  metadata/
    session-1731687523772-iqjwhjfia-transcription.json
    session-1731687523772-iqjwhjfia.json
    session-1731687589322-8r4i67ak1-transcription.json
    session-1731687589322-8r4i67ak1.json
    session-1731688034377-5wnbspbnh-transcription.json
    session-1731688034377-5wnbspbnh.json
    session-1731688189665-i25jbs76v-transcription.json
    session-1731688189665-i25jbs76v.json
    session-1731688243886-grtu7fqfa-transcription.json
    session-1731688243886-grtu7fqfa.json
    session-1731714341469-bokt8mv1t-transcription.json
    session-1731714341469-bokt8mv1t.json
    session-1731715333724-qsuqfxz74-transcription.json
    session-1731715333724-qsuqfxz74.json
    session-1731715712023-4ndrqiuus-transcription.json
    session-1731715712023-4ndrqiuus.json
    session-1731756619590-g1jd0yko1-transcription.json
    session-1731756619590-g1jd0yko1.json
    session-1731796422917-7khc3ky8x-transcription.json
    session-1731796422917-7khc3ky8x.json
    session-1731796721371-19ifajlcu-transcription.json
    session-1731796721371-19ifajlcu.json
    session-1731797118978-pg1npt5je-transcription.json
    session-1731797118978-pg1npt5je.json
    session-1731797976298-r7czxv3ju-transcription.json
    session-1731797976298-r7czxv3ju.json
    session-1732057400077-zqu6yf5cn-transcription.json
    session-1732057400077-zqu6yf5cn.json
    session-1732057719467-3stymg4-transcription.json
    session-1732057719467-3stymg4.json
    session-1732057770977-udehg9u-transcription.json
    session-1732057770977-udehg9u.json
    session-1732057776677-bfdsnja-transcription.json
    session-1732057776677-bfdsnja.json
    session-1732057849554-3m89hft-transcription.json
    session-1732057849554-3m89hft.json
    session-1732058087531-b8i4voq-transcription.json
    session-1732058087531-b8i4voq.json
    session-1732059156755-xc2ez90-transcription.json
    session-1732059156755-xc2ez90.json
    session-1732061955759-anhn84i-transcription.json
    session-1732061955759-anhn84i.json
  env.js
.eslintrc.json
.gitignore
file-watcher.js
llmtest.js
next.config.ts
package.json
postcss.config.mjs
printservice.py
README.md
server.mjs
tailwind.config.ts
test-file-watcher.js
test-generateimageprompt.js
test-metadata-permissions.js
test-transcription.mjs
testRecording.js
testWebSocketClient.js
tree_exporter.py
tsconfig.json

================================================================
Repository Files
================================================================

================
File: public/file.svg
================
<svg fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><path d="M14.5 13.5V5.41a1 1 0 0 0-.3-.7L9.8.29A1 1 0 0 0 9.08 0H1.5v13.5A2.5 2.5 0 0 0 4 16h8a2.5 2.5 0 0 0 2.5-2.5m-1.5 0v-7H8v-5H3v12a1 1 0 0 0 1 1h8a1 1 0 0 0 1-1M9.5 5V2.12L12.38 5zM5.13 5h-.62v1.25h2.12V5zm-.62 3h7.12v1.25H4.5zm.62 3h-.62v1.25h7.12V11z" clip-rule="evenodd" fill="#666" fill-rule="evenodd"/></svg>

================
File: public/globe.svg
================
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><g clip-path="url(#a)"><path fill-rule="evenodd" clip-rule="evenodd" d="M10.27 14.1a6.5 6.5 0 0 0 3.67-3.45q-1.24.21-2.7.34-.31 1.83-.97 3.1M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16m.48-1.52a7 7 0 0 1-.96 0H7.5a4 4 0 0 1-.84-1.32q-.38-.89-.63-2.08a40 40 0 0 0 3.92 0q-.25 1.2-.63 2.08a4 4 0 0 1-.84 1.31zm2.94-4.76q1.66-.15 2.95-.43a7 7 0 0 0 0-2.58q-1.3-.27-2.95-.43a18 18 0 0 1 0 3.44m-1.27-3.54a17 17 0 0 1 0 3.64 39 39 0 0 1-4.3 0 17 17 0 0 1 0-3.64 39 39 0 0 1 4.3 0m1.1-1.17q1.45.13 2.69.34a6.5 6.5 0 0 0-3.67-3.44q.65 1.26.98 3.1M8.48 1.5l.01.02q.41.37.84 1.31.38.89.63 2.08a40 40 0 0 0-3.92 0q.25-1.2.63-2.08a4 4 0 0 1 .85-1.32 7 7 0 0 1 .96 0m-2.75.4a6.5 6.5 0 0 0-3.67 3.44 29 29 0 0 1 2.7-.34q.31-1.83.97-3.1M4.58 6.28q-1.66.16-2.95.43a7 7 0 0 0 0 2.58q1.3.27 2.95.43a18 18 0 0 1 0-3.44m.17 4.71q-1.45-.12-2.69-.34a6.5 6.5 0 0 0 3.67 3.44q-.65-1.27-.98-3.1" fill="#666"/></g><defs><clipPath id="a"><path fill="#fff" d="M0 0h16v16H0z"/></clipPath></defs></svg>

================
File: public/next.svg
================
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 394 80"><path fill="#000" d="M262 0h68.5v12.7h-27.2v66.6h-13.6V12.7H262V0ZM149 0v12.7H94v20.4h44.3v12.6H94v21h55v12.6H80.5V0h68.7zm34.3 0h-17.8l63.8 79.4h17.9l-32-39.7 32-39.6h-17.9l-23 28.6-23-28.6zm18.3 56.7-9-11-27.1 33.7h17.8l18.3-22.7z"/><path fill="#000" d="M81 79.3 17 0H0v79.3h13.6V17l50.2 62.3H81Zm252.6-.4c-1 0-1.8-.4-2.5-1s-1.1-1.6-1.1-2.6.3-1.8 1-2.5 1.6-1 2.6-1 1.8.3 2.5 1a3.4 3.4 0 0 1 .6 4.3 3.7 3.7 0 0 1-3 1.8zm23.2-33.5h6v23.3c0 2.1-.4 4-1.3 5.5a9.1 9.1 0 0 1-3.8 3.5c-1.6.8-3.5 1.3-5.7 1.3-2 0-3.7-.4-5.3-1s-2.8-1.8-3.7-3.2c-.9-1.3-1.4-3-1.4-5h6c.1.8.3 1.6.7 2.2s1 1.2 1.6 1.5c.7.4 1.5.5 2.4.5 1 0 1.8-.2 2.4-.6a4 4 0 0 0 1.6-1.8c.3-.8.5-1.8.5-3V45.5zm30.9 9.1a4.4 4.4 0 0 0-2-3.3 7.5 7.5 0 0 0-4.3-1.1c-1.3 0-2.4.2-3.3.5-.9.4-1.6 1-2 1.6a3.5 3.5 0 0 0-.3 4c.3.5.7.9 1.3 1.2l1.8 1 2 .5 3.2.8c1.3.3 2.5.7 3.7 1.2a13 13 0 0 1 3.2 1.8 8.1 8.1 0 0 1 3 6.5c0 2-.5 3.7-1.5 5.1a10 10 0 0 1-4.4 3.5c-1.8.8-4.1 1.2-6.8 1.2-2.6 0-4.9-.4-6.8-1.2-2-.8-3.4-2-4.5-3.5a10 10 0 0 1-1.7-5.6h6a5 5 0 0 0 3.5 4.6c1 .4 2.2.6 3.4.6 1.3 0 2.5-.2 3.5-.6 1-.4 1.8-1 2.4-1.7a4 4 0 0 0 .8-2.4c0-.9-.2-1.6-.7-2.2a11 11 0 0 0-2.1-1.4l-3.2-1-3.8-1c-2.8-.7-5-1.7-6.6-3.2a7.2 7.2 0 0 1-2.4-5.7 8 8 0 0 1 1.7-5 10 10 0 0 1 4.3-3.5c2-.8 4-1.2 6.4-1.2 2.3 0 4.4.4 6.2 1.2 1.8.8 3.2 2 4.3 3.4 1 1.4 1.5 3 1.5 5h-5.8z"/></svg>

================
File: public/vercel.svg
================
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1155 1000"><path d="m577.3 0 577.4 1000H0z" fill="#fff"/></svg>

================
File: public/window.svg
================
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.5 2.5h13v10a1 1 0 0 1-1 1h-11a1 1 0 0 1-1-1zM0 1h16v11.5a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 0 12.5zm3.75 4.5a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5M7 4.75a.75.75 0 1 1-1.5 0 .75.75 0 0 1 1.5 0m1.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5" fill="#666"/></svg>

================
File: src/app/api/delete-memory-log/route.ts
================
import fs from 'fs/promises';
import path from 'path';

export async function POST() {
  const memoryLogPath = path.join(process.cwd(), 'metadata/memory-log.txt');
  console.log('API invoked: /api/delete-memory-log');
  console.log('Resolved memory log path:', memoryLogPath);

  try {
    // Check if file exists
    await fs.access(memoryLogPath);
    console.log('File exists, attempting to delete...');

    // Attempt to delete the file
    await fs.unlink(memoryLogPath);
    console.log('Memory log deleted successfully.');
    return new Response(JSON.stringify({ success: true }), { status: 200 });
  } catch (error) {
    console.error('Error during deletion:', error);

    if ((error as NodeJS.ErrnoException).code === 'ENOENT') {
      console.warn('File not found, returning success response.');
      return new Response(
        JSON.stringify({ success: true, message: 'File already deleted' }),
        { status: 200 }
      );
    }

    return new Response(
      JSON.stringify({ error: 'Failed to delete memory log' }),
      { status: 500 }
    );
  }
}

================
File: src/app/api/images/route.ts
================
import { NextResponse } from 'next/server';
import { join } from 'path';
import { readdirSync } from 'fs';

export async function GET() {
  try {
    const imagesDirPath = join(process.cwd(), 'generated_images');
    const files = readdirSync(imagesDirPath)
      .filter(file => file.endsWith('.png'))
      .sort((a, b) => {
        const getTimestamp = (filename: string) => {
          const match = filename.match(/-(\d+)\.png$/);
          return match ? parseInt(match[1]) : 0;
        };
        return getTimestamp(a) - getTimestamp(b);
      });

    return NextResponse.json({ 
      images: files.map(file => `/api/images/${file}`)
    });
  } catch (error) {
    console.error('Error reading images directory:', error);
    return NextResponse.json({ error: 'Failed to read images' }, { status: 500 });
  }
}

================
File: src/app/api/print/route.ts
================
import { NextResponse } from 'next/server';
import { spawn } from 'child_process';

export async function POST(request: Request) {
  try {
    const body = await request.json();
    const { imageName } = body;

    if (!imageName) {
      return NextResponse.json({ error: 'Image name is required' }, { status: 400 });
    }

    return new Promise((resolve, reject) => {
      // Spawn the Python process
      const process = spawn('python3', ['printservice.py', imageName]);

      process.stdout.on('data', (data) => {
        console.log(`stdout: ${data}`);
      });

      process.stderr.on('data', (data) => {
        console.error(`stderr: ${data}`);
      });

      process.on('close', (code) => {
        if (code === 0) {
          console.log('Python script executed successfully');
          resolve(
            NextResponse.json({ status: 'Print job started successfully' }, { status: 200 })
          );
        } else {
          console.error(`Python script exited with code ${code}`);
          reject(NextResponse.json({ error: 'Python script execution failed' }, { status: 500 }));
        }
      });
    });
  } catch (error) {
    console.error('Error executing print job:', error);
    return NextResponse.json({ error: 'Internal server error' }, { status: 500 });
  }
}

================
File: src/app/api/socket/route.ts
================
// src/app/api/socket/route.ts
import { NextResponse } from 'next/server';
import { Server as SocketIOServer } from 'socket.io';

const globalForIO = global as { io?: SocketIOServer };

export async function GET() {
  console.log('WebSocket route handler called');
  
  if (!globalForIO.io) {
    console.log('Creating new Socket.IO server');
    
    const io = new SocketIOServer({
      cors: {
        origin: ["http://localhost:3000", "http://localhost:3001"],
        methods: ["GET", "POST"],
        credentials: true
      },
      transports: ['websocket', 'polling'],
      path: '/api/socket'
    });

    globalForIO.io = io;
    console.log('Socket.IO server created');

    io.on('connection', (socket) => {
      console.log('Client connected:', socket.id);

      socket.on('audioChunk', (chunk: ArrayBuffer) => {
        console.log('Received audio chunk of size:', chunk.byteLength);
      });

      socket.on('error', (error) => {
        console.error('Socket error:', error);
      });

      socket.on('disconnect', () => {
        console.log('Client disconnected:', socket.id);
      });
    });
  } else {
    console.log('Socket.IO server already exists');
  }

  return new NextResponse('WebSocket server is running', {
    status: 200,
    headers: {
      'Content-Type': 'text/plain',
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'GET, POST',
    },
  });
}

================
File: src/app/api/transcription/[sessionId]/route.ts
================
import { NextRequest, NextResponse } from 'next/server';
import fs from 'fs/promises';
import path from 'path';

// Get the absolute path to metadata directory
const METADATA_DIR = path.join(process.cwd(), 'metadata');

export async function GET(req: NextRequest, { params }: { params: { sessionId: string } }) {
    const { sessionId } = params;
    console.log('Looking for transcription file:', sessionId);
    const transcriptionPath = path.join(METADATA_DIR, `${sessionId}-transcription.json`);
    console.log('Full path:', transcriptionPath);

    try {
        const data = await fs.readFile(transcriptionPath, 'utf-8');
        return NextResponse.json(JSON.parse(data));
    } catch (error) {
        console.error(`Error fetching transcription for session ${sessionId}:`, error);
        return NextResponse.json({ error: 'Transcription not found' }, { status: 404 });
    }
}

================
File: src/app/session/[sessionId]/page.tsx
================
'use client';

import { PlayCircle, StopCircle, RotateCcw, Printer } from 'lucide-react';
import { useAudioRecorder } from '@/lib/hooks/useAudioRecorder';
import { useState, useEffect, useCallback, useRef } from 'react';
import { useParams, useRouter } from 'next/navigation';
import TranscriptionViewer from '@/components/transcription/TranscriptionViewer';
import ImageDisplay from '@/components/ImageDisplay';
import { cn } from '@/lib/utils/ui';
import RecordingProgress from '@/components/RecordingProgress';
import WelcomeOverlay from '@/components/WelcomeOverlay';



const SessionPage = () => {
  const params = useParams();
  const router = useRouter();
  const sessionIdFromUrl = params?.sessionId as string;

  const {
    isRecording,
    startRecording,
    stopRecording,
    sessionData,
    transcriptions,
    isConnected,
    error: hookError,
    startSession,
    sendWebSocketMessage,
  } = useAudioRecorder();
  const [error, setError] = useState<string | null>(null);
  const [isFullscreen, setIsFullscreen] = useState(false);
  const [currentImageUrl, setCurrentImageUrl] = useState<string | null>(null);
  const [isPrinting, setIsPrinting] = useState(false); // Add this here
  const [progress, setProgress] = useState(0); // Progress bar state
  const [showWelcome, setShowWelcome] = useState(true);
  const idleTimerRef = useRef<NodeJS.Timeout | null>(null);
  const IDLE_TIMEOUT = 90000; // 90 seconds in milliseconds



   // Add the debug useEffect here
   useEffect(() => {
    console.log('Current image URL updated:', currentImageUrl);
  }, [currentImageUrl]);


  const handleRestart = useCallback(async () => {
    try {
      setError(null);
  
      if (isRecording) {
        stopRecording();
      }
  
      // First delete memory log
      const response = await fetch('/api/delete-memory-log', { method: 'POST' });
      if (!response.ok) {
        throw new Error('Failed to delete memory log');
      }
  
      // Send reset message through existing WebSocket connection
      const message = {
        type: 'session_reset',
        timestamp: Date.now(),
        sessionId: sessionData?.sessionId || ''
      };
  
      if (isConnected) {
        // Send through existing hook's WebSocket connection
        sendWebSocketMessage(message);
        console.log('Sent session reset message');
      } else {
        console.warn('WebSocket not connected for session reset');
      }
  
      console.log('Memory log deleted and reset triggered');
  
      const newSessionId = await startSession();
      if (newSessionId) {
        router.push(`/session/${newSessionId}`);
      }
    } catch (err) {
      console.error('Failed to restart session:', err);
      setError(err instanceof Error ? err.message : 'Failed to restart session');
    }
  }, [isRecording, stopRecording, startSession, router, sessionData?.sessionId, isConnected, sendWebSocketMessage]);

  const handlePrint = useCallback(async () => {
    if (!currentImageUrl) {
      console.error('No image available to print');
      return;
    }
  
    setError(null);
    setIsPrinting(true);
    setProgress(0);
  
    const imageName = currentImageUrl.split('/').pop();
  
    // Start the progress bar independently
    const progressInterval = setInterval(() => {
      setProgress((prev) => (prev < 100 ? prev + 1 : 100));
    }, 600); // Update every 0.6 seconds for 60 seconds (or adjust as needed)
  
    try {
      const response = await fetch('/api/print', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ imageName }),
      });
  
      if (!response.ok) {
        throw new Error('Failed to start print job');
      }
  
      const result = await response.json();
      console.log('Print job started:', result);
    } catch (error) {
      console.error('Error starting print job:', error);
      setError('Failed to print image. Please try again.');
    } finally {
      clearInterval(progressInterval); // Stop the progress bar updates
      setProgress(100); // Ensure the progress bar completes
      setTimeout(() => setIsPrinting(false), 1000); // Allow the progress bar to complete visually before hiding the overlay
    }
  }, [currentImageUrl]);  

  const toggleRecording = useCallback(() => {
    if (isRecording) {
      stopRecording();
    } else if (isConnected) {
      startRecording();
      setShowWelcome(false);  // Hide welcome screen when starting recording
    }
  }, [isRecording, isConnected, startRecording, stopRecording]);

  useEffect(() => {
    const handleKeydown = (event: KeyboardEvent) => {
      switch (event.key.toLowerCase()) {
        case '1': // Toggle recording
          toggleRecording();
          break;
        case '3': // Print the current image
          handlePrint();
          break;
        case '2': // Restart session
          handleRestart();
          break;
        default:
          break;
      }
    };

    window.addEventListener('keydown', handleKeydown);
    return () => {
      window.removeEventListener('keydown', handleKeydown);
    };
  }, [toggleRecording, handlePrint, handleRestart]);

  useEffect(() => {
    if (sessionIdFromUrl && !sessionData) {
      console.log('Connecting to existing session:', sessionIdFromUrl);
      startSession(sessionIdFromUrl).catch((err) => {
        console.warn('Error connecting to session:', err);

        if (err.message.includes('WebSocket is not connected')) {
          console.warn('Suppressing session connection error');
          return;
        }

        setError('Failed to connect to session');
      });
    }
  }, [sessionIdFromUrl, sessionData, startSession]);

  const resetIdleTimer = useCallback(() => {
    if (idleTimerRef.current) {
      clearTimeout(idleTimerRef.current);
    }
    
    // Only start idle timer if we're not showing welcome screen and not recording
    if (!showWelcome && !isRecording) {
      idleTimerRef.current = setTimeout(() => {
        setShowWelcome(true);
        // Call handleRestart when timeout occurs
        handleRestart();
      }, IDLE_TIMEOUT);
    }
  }, [showWelcome, isRecording, handleRestart]);
  
  // Add these effect hooks
  useEffect(() => {
    // Reset timer on any user interaction
    const handleUserActivity = () => {
      resetIdleTimer();
    };
  
    // Add event listeners for user activity
    window.addEventListener('mousemove', handleUserActivity);
    window.addEventListener('keydown', handleUserActivity);
    window.addEventListener('click', handleUserActivity);
  
    // Start initial timer
    resetIdleTimer();
  
    // Cleanup
    return () => {
      if (idleTimerRef.current) {
        clearTimeout(idleTimerRef.current);
      }
      window.removeEventListener('mousemove', handleUserActivity);
      window.removeEventListener('keydown', handleUserActivity);
      window.removeEventListener('click', handleUserActivity);
    };
  }, [resetIdleTimer]);
  
  // Reset idle timer when recording starts/stops
  useEffect(() => {
    resetIdleTimer();
  }, [isRecording, resetIdleTimer]);
  
  return (
    <main className="h-screen w-full bg-gray-900 overflow-hidden">
      {/* Overlay - place first */}
      {showWelcome && !isRecording && <WelcomeOverlay />}
      {/* Printing in Progress Overlay */}
      {isPrinting && (
        <div className="absolute inset-0 z-50 flex flex-col items-center justify-center bg-black/75 text-white text-lg">
          <p>Printing in progress. Please wait...</p>
          <div className="w-3/4 mt-4 h-4 bg-gray-700 rounded">
            <div
              className="h-4 bg-green-500 rounded"
              style={{ width: `${progress}%` }}
            ></div>
          </div>
        </div>
      )}
  
      {/* Error Message */}
      {error || hookError ? (
        <div className="absolute top-4 left-1/2 -translate-x-1/2 z-50 rounded-lg bg-red-900/50 p-4 text-red-200 text-sm">
          {error || hookError}
        </div>
      ) : null}
  
      <div className="flex h-full">
        <div
          className={cn(
            'transition-all duration-300 ease-in-out',
            isFullscreen ? 'w-0 opacity-0' : 'w-1/3 opacity-100'
          )}
        >
          <div className="h-full overflow-auto p-4 space-y-4">
            <h2 className="text-2xl font-obra text-gray-100">Session Recording</h2>
            <p className="text-gray-400 mt-2 font-obra">
              Control and manage your D&D session recordings.
            </p>
  
            <div className="flex items-center space-x-4">
              <button
                onClick={toggleRecording}
                disabled={!isConnected}
                className={cn(
                  'p-4 rounded-lg text-white transition-all',
                  isRecording
                    ? 'bg-red-700 hover:bg-red-800'
                    : 'bg-green-700 hover:bg-green-800'
                )}
              >
                {isRecording ? (
                  <>
                    <StopCircle className="w-6 h-6 inline-block mr-2" />
                    Stop Recording
                  </>
                ) : (
                  <>
                    <PlayCircle className="w-6 h-6 inline-block mr-2" />
                    Start Recording
                  </>
                )}
              </button>
              <button
                onClick={handleRestart}
                className="p-4 rounded-lg bg-yellow-700 text-white hover:bg-yellow-800 transition-all"
              >
                <RotateCcw className="w-6 h-6 inline-block mr-2" />
                Restart Session
              </button>
              <button
                onClick={handlePrint}
                disabled={!currentImageUrl || isPrinting}
                className="p-4 rounded-lg bg-blue-700 text-white hover:bg-blue-800 transition-all disabled:opacity-50 disabled:cursor-not-allowed"
              >
                <Printer className="w-6 h-6 inline-block mr-2" />
                {isPrinting ? 'Printing...' : 'Print Image'}
              </button>
            </div>
  
            <TranscriptionViewer
              sessionId={sessionData?.sessionId || ''}
              isRecording={isRecording}
              sessionActive={!!sessionData}
              transcriptions={transcriptions}
            />
          </div>
        </div>
  
        <div
          className={cn(
            'transition-all duration-300 ease-in-out',
            isFullscreen ? 'w-full' : 'w-2/3'
          )}
        >
          <ImageDisplay
            sessionId={sessionData?.sessionId || ''}
            isFullscreen={isFullscreen}
            onToggleFullscreen={() => setIsFullscreen(!isFullscreen)}
            isRecording={isRecording}
            onImageChange={setCurrentImageUrl}
          />
        </div>
      </div>
      {isRecording && <RecordingProgress />}
    </main>
  );
};

export default SessionPage;

================
File: src/app/session/page.tsx
================
'use client';

import { useRouter } from 'next/navigation';
import { PlayCircle, Images, Loader2 } from 'lucide-react';
import { useAudioRecorder } from '@/lib/hooks/useAudioRecorder';
import { useState } from 'react';

export default function Home() {
  const router = useRouter();
  const { startSession, isConnected, error: hookError } = useAudioRecorder();
  const [error, setError] = useState<string | null>(null);
  const [isLoading, setIsLoading] = useState(false);

  const handleStartSession = async () => {
    try {
      setIsLoading(true);
      setError(null);
      
      if (!isConnected) {
        throw new Error('Please wait for connection to be established');
      }
      
      const sessionId = await startSession();
      // Wait for session to be established
      await new Promise(resolve => setTimeout(resolve, 500));
      
      if (sessionId) {
        router.push(`/session/${sessionId}`);
      } else {
        throw new Error('Failed to create session');
      }
    } catch (err) {
      console.error('Failed to start session:', err);
      setError(err instanceof Error ? err.message : 'Failed to start session');
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <main className="flex-1 p-6">
      <div className="max-w-7xl mx-auto space-y-8">
        <div>
          <h2 className="text-2xl font-bold text-gray-800">Welcome to Mythra</h2>
          <p className="text-gray-600 mt-2">Start a new session or view past recordings.</p>
        </div>

        {(error || hookError) && (
          <div className="rounded-lg bg-red-50 p-4 text-red-700 text-sm">
            {error || hookError}
          </div>
        )}

        <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
          <button 
            onClick={handleStartSession}
            disabled={isLoading || !isConnected}
            className="p-8 rounded-xl border bg-white shadow-sm hover:shadow-md transition-all duration-200 hover:scale-[1.02] text-left group relative"
          >
            <div className="flex items-center gap-3 mb-2">
              {isLoading ? (
                <Loader2 className="w-6 h-6 text-blue-600 animate-spin" />
              ) : (
                <PlayCircle className="w-6 h-6 text-green-600 group-hover:scale-110 transition-transform" />
              )}
              <h3 className="text-lg font-semibold text-gray-800">
                {isLoading ? 'Starting Session...' : 'Start New Session'}
              </h3>
            </div>
            <p className="text-sm text-gray-600">Begin recording a new D&D session with AI-powered visualizations.</p>
            {!isConnected && (
              <div className="absolute inset-0 bg-gray-50/50 flex items-center justify-center rounded-xl">
                <div className="flex items-center gap-2 text-sm text-gray-600">
                  <Loader2 className="w-4 h-4 animate-spin" />
                  Connecting to server...
                </div>
              </div>
            )}
          </button>

          <button 
            onClick={() => router.push('/recaps')}
            className="p-8 rounded-xl border bg-white shadow-sm hover:shadow-md transition-all duration-200 hover:scale-[1.02] text-left group"
          >
            <div className="flex items-center gap-3 mb-2">
              <Images className="w-6 h-6 text-purple-600 group-hover:scale-110 transition-transform" />
              <h3 className="text-lg font-semibold text-gray-800">View Recaps</h3>
            </div>
            <p className="text-sm text-gray-600">Browse through past session recaps and generated imagery.</p>
          </button>
        </div>
      </div>
    </main>
  );
}

================
File: src/app/fonts.ts
================
import localFont from 'next/font/local'

export const obraLetra = localFont({
  src: './fonts/ObraLetra.ttf',
  variable: '--font-obra',
  display: 'swap',
})

// Keep your existing fonts if you're using them
export const geistMono = localFont({
  src: './fonts/GeistMonoVF.woff',
  variable: '--font-geist-mono',
  display: 'swap',
})

export const geist = localFont({
  src: './fonts/GeistVF.woff',
  variable: '--font-geist',
  display: 'swap',
})

================
File: src/app/globals.css
================
@tailwind base;
@tailwind components;
@tailwind utilities;


@font-face {
  font-family: 'Obra Letra';
  src: url('/fonts/ObraLetra.ttf') format('truetype');
  font-weight: normal;
  font-style: normal;
  font-display: swap;
}

@layer base {
  body {
    @apply bg-gray-900 text-gray-100;
    font-family: 'Obra Letra', system-ui, sans-serif;
  }

  /* Ensure all headings use the font */
  h1, h2, h3, h4, h5, h6 {
    font-family: 'Obra Letra', system-ui, sans-serif;
  }
}

:root {
  --background: #ffffff;
  --foreground: #171717;
}

@media (prefers-color-scheme: dark) {
  :root {
    --background: #0a0a0a;
    --foreground: #ededed;
  }
}

body {
  color: var(--foreground);
  background: var(--background);
  font-family: Arial, Helvetica, sans-serif;
}

.button-spacing {
  margin-bottom: 16px; /* Adjust the spacing as needed */
}

================
File: src/app/layout.tsx
================
// src/app/layout.tsx
import './globals.css';
import LayoutClient from '@/components/layout/LayoutClient';
import type { ReactNode } from 'react';
import { obraLetra, geist, geistMono } from './fonts';

export default function RootLayout({
  children,
}: {
  children: ReactNode;
}) {
  return (
    <html lang="en" className={`${obraLetra.variable} ${geist.variable} ${geistMono.variable}`}>
      <body className="bg-gray-900 font-obra">
        <LayoutClient>
          {children}
        </LayoutClient>
      </body>
    </html>
  );
}

================
File: src/app/page.tsx
================
'use client';

import { useRouter } from 'next/navigation';
import { PlayCircle, Images } from 'lucide-react';
import { useAudioRecorder } from '@/lib/hooks/useAudioRecorder';

export default function Home() {
  const router = useRouter();
  const { startSession } = useAudioRecorder();

  const handleStartSession = async () => {
    try {
      const sessionId = await startSession();
      router.push(`/session/${sessionId}`);
    } catch (err) {
      console.error('Failed to start session:', err);
    }
  };

  return (
    <main className="flex-1 p-6">
      <div className="max-w-7xl mx-auto space-y-8">
        <div>
          <h2 className="text-2xl font-obra text-gray-100">Welcome to Mythra</h2>
          <p className="text-gray-400 mt-2 font-obra">Start a new session or view past recordings.</p>
        </div>

        <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
          <button 
            onClick={handleStartSession}
            className="p-8 rounded-xl border border-gray-700 bg-gray-800/50 shadow-lg shadow-black/20 hover:shadow-xl hover:shadow-black/30 hover:bg-gray-800/80 transition-all duration-200 hover:scale-[1.02] text-left group backdrop-blur-sm"
          >
            <div className="flex items-center gap-3 mb-2">
              <PlayCircle className="w-6 h-6 text-emerald-400 group-hover:scale-110 transition-transform" />
              <h3 className="text-lg font-obra text-gray-100">Start New Session</h3>
            </div>
            <p className="text-sm text-gray-400 font-obra">Begin recording a new D&D session with AI-powered visualizations.</p>
          </button>

          <button 
            onClick={() => router.push('/recaps')}
            className="p-8 rounded-xl border border-gray-700 bg-gray-800/50 shadow-lg shadow-black/20 hover:shadow-xl hover:shadow-black/30 hover:bg-gray-800/80 transition-all duration-200 hover:scale-[1.02] text-left group backdrop-blur-sm"
          >
            <div className="flex items-center gap-3 mb-2">
              <Images className="w-6 h-6 text-blue-400 group-hover:scale-110 transition-transform" />
              <h3 className="text-lg font-obra text-gray-100">View Recaps</h3>
            </div>
            <p className="text-sm text-gray-400 font-obra">Browse through past session recaps and generated imagery.</p>
          </button>
        </div>
      </div>
    </main>
  );
}

================
File: src/components/layout/AudioLevelIndicator.tsx
================
// src/components/layout/AudioLevelIndicator.tsx
'use client';
import React from 'react';

interface AudioLevelIndicatorProps {
  level: number;
  isRecording: boolean;
}

const AudioLevelIndicator = ({ level = 0, isRecording = false }: AudioLevelIndicatorProps) => {
  // Create 10 segments for the level indicator
  const segments = 10;
  const segmentLevel = Math.floor((level / 60) * segments);
  
  return (
    <div className="space-y-1">
      <div className="flex items-center gap-2">
        <div className={`w-2 h-2 rounded-full transition-colors ${
          isRecording ? 'bg-red-500 animate-pulse' : 'bg-gray-300'
        }`} />
        <span className="text-xs text-gray-500">
          {isRecording ? 'Recording' : 'Ready'}
        </span>
      </div>
      
      <div className="flex gap-0.5 h-4">
        {Array.from({ length: segments }).map((_, i) => (
          <div
            key={i}
            className={`flex-1 rounded-sm transition-all ${
              i < segmentLevel
                ? i < segments * 0.6
                  ? 'bg-green-500'
                  : i < segments * 0.8
                    ? 'bg-yellow-500'
                    : 'bg-red-500'
                : 'bg-gray-200'
            }`}
          />
        ))}
      </div>
    </div>
  );
};

export default AudioLevelIndicator;

================
File: src/components/layout/Footer.tsx
================
// src/components/layout/Footer.tsx
'use client';
import { usePathname } from 'next/navigation';
import { Wifi, WifiOff, Mic} from 'lucide-react';
import { useAudioRecorder } from '@/lib/hooks/useAudioRecorder';

export default function Footer() {
  const pathname = usePathname();
  const { 
    isConnected,
    isRecording,
    sessionData
  } = useAudioRecorder();

  const formatDuration = (ms: number) => {
    if (!ms) return '00:00';
    const seconds = Math.floor(ms / 1000);
    const minutes = Math.floor(seconds / 60);
    const remainingSeconds = seconds % 60;
    return `${minutes}:${remainingSeconds.toString().padStart(2, '0')}`;
  };

  const showSessionInfo = pathname === '/session';

  return (
    <footer className="border-t border-gray-700 bg-gray-900/80 backdrop-blur-sm">
      <div className="px-8 py-4 flex justify-between items-center">
        <div className="flex items-center gap-4">
          {/* Connection Status */}
          <div className="flex items-center gap-2 text-sm">
            {isConnected ? (
              <>
                <Wifi className="w-4 h-4 text-emerald-400" />
                <span className="text-emerald-400">Connected</span>
              </>
            ) : (
              <>
                <WifiOff className="w-4 h-4 text-amber-400 animate-pulse" />
                <span className="text-amber-400">Connecting...</span>
              </>
            )}
          </div>

          {/* Recording Status - Only show on session page */}
          {showSessionInfo && sessionData?.status && (
            <div className="flex items-center gap-2 text-sm">
              <div className="w-px h-4 bg-gray-700" /> {/* Divider */}
              <Mic className={`w-4 h-4 ${isRecording ? 'text-red-400 animate-pulse' : 'text-gray-400'}`} />
              <span className="capitalize text-gray-300">{sessionData.status}</span>
              {isRecording && sessionData.totalDuration > 0 && (
                <span className="text-gray-300">{formatDuration(sessionData.totalDuration)}</span>
              )}
            </div>
          )}

          {/* Show session ID if available */}
          {showSessionInfo && sessionData?.sessionId && (
            <div className="flex items-center gap-2 text-sm">
              <div className="w-px h-4 bg-gray-700" /> {/* Divider */}
              <span className="text-gray-400">Session: {sessionData.sessionId}</span>
            </div>
          )}
        </div>

        <div className="text-sm text-gray-500">
          Mythra v0.1
        </div>
      </div>
    </footer>
  );
}

================
File: src/components/layout/Header.tsx
================
// src/components/layout/Header.tsx
'use client';
import { usePathname, useRouter } from 'next/navigation';
import { Mic, Brain, Book } from 'lucide-react';

export default function Header() {
  const pathname = usePathname();
  const router = useRouter();

  return (
    <header className="border-b border-gray-700 bg-gray-900/80 backdrop-blur-sm sticky top-0 z-10">
      <div className="flex items-center justify-between h-16 px-8">
        <div className="flex items-center space-x-4">
          <h1 
            onClick={() => router.push('/')} 
            className="text-xl font-obra text-blue-400 hover:text-blue-300 cursor-pointer transition-colors"
          >
            Mythra
          </h1>
          <nav className="hidden md:flex space-x-6">
            <button 
              onClick={() => router.push('/session')}
              className={`text-sm font-obra transition-colors flex items-center gap-2 
                ${pathname === '/session' ? 'text-blue-400' : 'text-gray-400 hover:text-blue-400'}`}
            >
              <Mic className="w-4 h-4" />
              Session
            </button>
            <button 
              onClick={() => router.push('/memory')}
              className={`text-sm font-obra transition-colors flex items-center gap-2 
                ${pathname === '/memory' ? 'text-blue-400' : 'text-gray-400 hover:text-blue-400'}`}
            >
              <Brain className="w-4 h-4" />
              Memory
            </button>
            <button 
              onClick={() => router.push('/recaps')}
              className={`text-sm font-obra transition-colors flex items-center gap-2 
                ${pathname === '/recaps' ? 'text-blue-400' : 'text-gray-400 hover:text-blue-400'}`}
            >
              <Book className="w-4 h-4" />
              Recaps
            </button>
          </nav>
        </div>
      </div>
    </header>
  );
}

================
File: src/components/layout/LayoutClient.tsx
================
// src/components/layout/LayoutClient.tsx
'use client';

import Header from '@/components/layout/Header';
import Footer from '@/components/layout/Footer';
import type { ReactNode } from 'react';

interface LayoutClientProps {
  children: ReactNode;
}

export default function LayoutClient({ children }: LayoutClientProps) {
  return (
    <div className="flex flex-col h-screen">
      <Header />
      <div className="flex-1 flex flex-col overflow-auto">
        {children}
        <Footer />
      </div>
    </div>
  );
}

================
File: src/components/layout/MainContent.tsx
================
import TranscriptionViewer from '@/components/transcription/TranscriptionViewer';

export default function MainContent({ children }: { children: React.ReactNode }) {
  return (
    <main className='flex-1 p-6'>
      <div className='max-w-7xl mx-auto'>{children}</div>
    </main>
  );
}

================
File: src/components/layout/RecordingControls.tsx
================
// src/components/layout/RecordingControls.tsx
'use client';
import React from 'react';
import { Mic, AlertCircle } from 'lucide-react';
import { useAudioRecorder } from '@/lib/hooks/useAudioRecorder';
import AudioLevelIndicator from './AudioLevelIndicator';

const RecordingControls = () => {
  const { 
    isRecording, 
    startRecording, 
    stopRecording, 
    error, 
    audioLevel,
    isConnected,
    sessionData 
  } = useAudioRecorder();

  const formatDuration = (ms: number) => {
    const seconds = Math.floor(ms / 1000);
    const minutes = Math.floor(seconds / 60);
    const remainingSeconds = seconds % 60;
    return `${minutes}:${remainingSeconds.toString().padStart(2, '0')}`;
  };

  const formatFileSize = (bytes: number) => {
    if (bytes < 1024) return `${bytes} B`;
    if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)} KB`;
    return `${(bytes / (1024 * 1024)).toFixed(1)} MB`;
  };

  return (
    <div className="space-y-4">
      <div className="flex items-center justify-between">
        <h3 className="font-medium">Recording Controls</h3>
        <div className="flex items-center gap-2">
          <span className={`w-2 h-2 rounded-full ${isConnected ? 'bg-green-500' : 'bg-yellow-500 animate-pulse'}`} />
          <span className="text-sm text-gray-500">
            {isConnected ? 'Connected' : 'Connecting...'}
          </span>
        </div>
      </div>

      {/* Session Status */}
      {sessionData && (
        <div className="space-y-2 text-sm text-gray-600">
          <div className="flex justify-between">
            <span>Duration:</span>
            <span>{formatDuration(sessionData.totalDuration)}</span>
          </div>
          <div className="flex justify-between">
            <span>Size:</span>
            <span>{formatFileSize(sessionData.totalSize)}</span>
          </div>
          <div className="flex justify-between">
            <span>Status:</span>
            <span className="capitalize">{sessionData.status}</span>
          </div>
        </div>
      )}

      {/* Audio Level Indicator */}
      <div className="space-y-2">
        <AudioLevelIndicator 
          level={audioLevel} 
          isRecording={isRecording} 
        />
      </div>

      {/* Error Display */}
      {error && (
        <div className="flex items-center gap-2 p-2 text-sm text-red-600 bg-red-50 rounded-md">
          <AlertCircle className="w-4 h-4" />
          <p>{error}</p>
        </div>
      )}

      {/* Recording Button */}
      <button 
        onClick={isRecording ? stopRecording : startRecording}
        disabled={!isConnected}
        className={`
          w-full flex items-center justify-center gap-2 px-4 py-2 
          text-sm font-medium text-white rounded-md transition-all
          ${isRecording 
            ? 'bg-red-600 hover:bg-red-700' 
            : 'bg-green-600 hover:bg-green-700'
          }
          ${!isConnected && 'opacity-50 cursor-not-allowed'}
        `}
      >
        <Mic className={`w-4 h-4 ${isRecording ? 'animate-pulse' : ''}`} />
        {isRecording ? 'Stop Recording' : 'Start Recording'}
      </button>
    </div>
  );
};

export default RecordingControls;

================
File: src/components/layout/Sidebar.tsx
================
// Sidebar.tsx
"use client";
import { Mic, ImagePlus, Wifi, WifiOff } from 'lucide-react'
import { useAudioRecorder } from '@/lib/hooks/useAudioRecorder'
import AudioLevelIndicator from './AudioLevelIndicator'

console.log("Sidebar component loaded");

export default function Sidebar() {
  const { 
    isRecording, 
    startRecording, 
    stopRecording, 
    error, 
    audioLevel,
    isConnected // New WebSocket status
  } = useAudioRecorder()
  
  const handleRecordingClick = async () => {
    if (isRecording) {
      stopRecording()
    } else {
      await startRecording()
    }
  }

  return (
    <aside className="w-64 border-r h-screen bg-white shadow-sm">
      <div className="p-4">
        <div className="flex justify-between items-center mb-6">
          <h2 className="text-lg font-semibold text-gray-800">Controls</h2>
          {/* Connection Status Indicator */}
          <div className="flex items-center gap-2">
            {isConnected ? (
              <Wifi className="w-4 h-4 text-green-600" />
            ) : (
              <WifiOff className="w-4 h-4 text-yellow-600 animate-pulse" />
            )}
          </div>
        </div>
        <div className="space-y-6">
          <div className="p-6 rounded-xl border shadow-sm hover:shadow-md transition-all duration-200">
            <h3 className="text-sm font-medium mb-3 text-gray-700">Recording Status</h3>
            
            {/* Audio Level Indicator */}
            <div className="mb-3">
              <AudioLevelIndicator 
                level={audioLevel} 
                isRecording={isRecording} 
              />
            </div>

            {/* Connection Status Message */}
            {!isConnected && (
              <p className="text-sm text-yellow-600 mb-3">
                Connecting to server...
              </p>
            )}

            {/* Error Display */}
            {error && (
              <p className="text-sm text-red-600 mb-3">{error}</p>
            )}
            
            <button 
              onClick={handleRecordingClick}
              disabled={!isConnected}
              className={`w-full px-4 py-2 text-sm font-medium text-white rounded-md transition-colors flex items-center justify-center gap-2 group
                ${isRecording 
                  ? 'bg-red-600 hover:bg-red-700' 
                  : 'bg-green-600 hover:bg-green-700'
                }
                ${!isConnected && 'opacity-50 cursor-not-allowed'}
              `}
            >
              <Mic className={`w-4 h-4 group-hover:scale-110 transition-transform ${isRecording ? 'animate-pulse' : ''}`} />
              {isRecording ? 'Stop Recording' : 'Start Recording'}
            </button>
          </div>
          
          <div className="p-6 rounded-xl border shadow-sm hover:shadow-md transition-all duration-200">
            <h3 className="text-sm font-medium mb-3 text-gray-700">Image Generation</h3>
            <button 
              className="w-full px-4 py-2 text-sm font-medium text-white bg-purple-600 rounded-md hover:bg-purple-700 transition-colors flex items-center justify-center gap-2 group"
              disabled={!isConnected}
            >
              <ImagePlus className="w-4 h-4 group-hover:scale-110 transition-transform" />
              Enable Images
            </button>
          </div>
        </div>
      </div>
    </aside>
  )
}

================
File: src/components/transcription/TranscriptionViewer.tsx
================
'use client';

import React, { useEffect, useRef } from 'react';
import { Clock } from 'lucide-react';
import { Card } from "@/components/ui/card";
import type { TranscriptionData } from '@/lib/types/audio';

interface TranscriptionViewerProps {
  sessionId: string;
  isRecording: boolean;
  sessionActive: boolean;
  transcriptions: TranscriptionData[];
}

export default function TranscriptionViewer({ 
  sessionId,
  isRecording,
  sessionActive,
  transcriptions 
}: TranscriptionViewerProps) {
  const containerRef = useRef<HTMLDivElement>(null);

  // Enhanced debug effect to track all state changes
  useEffect(() => {
    console.log('TranscriptionViewer: State Update:', {
      sessionId,
      isRecording,
      sessionActive,
      transcriptionsLength: transcriptions?.length,
      transcriptions: transcriptions || [],
      isArrayValid: Array.isArray(transcriptions),
      transcriptionTexts: transcriptions?.map(t => t.text),
      hasContainer: !!containerRef.current
    });
  }, [sessionId, isRecording, sessionActive, transcriptions]);

  // Auto-scroll effect with debug
  useEffect(() => {
    console.log('TranscriptionViewer: Auto-scroll effect triggered', {
      hasTranscriptions: !!transcriptions?.length,
      containerExists: !!containerRef.current,
      currentScroll: containerRef.current?.scrollTop,
      scrollHeight: containerRef.current?.scrollHeight
    });

    if (containerRef.current && transcriptions?.length > 0) {
      const scrollTarget = containerRef.current.scrollHeight;
      containerRef.current.scrollTop = scrollTarget;
      
      console.log('TranscriptionViewer: Scrolled to:', {
        target: scrollTarget,
        actual: containerRef.current.scrollTop
      });
    }
  }, [transcriptions]);

  const formatTimestamp = (timestamp: number) => {
    const date = new Date(timestamp);
    return date.toLocaleTimeString([], { 
      hour: '2-digit', 
      minute: '2-digit',
      second: '2-digit',
      hour12: false 
    });
  };

  if (!transcriptions?.length) {
    return (
      <Card className="bg-gray-900 border-gray-700">
        <div className="rounded-lg bg-gray-900 p-8 text-center text-gray-400">
          {isRecording ? (
            <div className="flex items-center justify-center gap-2">
              <div className="h-2 w-2 rounded-full bg-red-500 animate-pulse" />
              <p>Waiting for transcription...</p>
            </div>
          ) : (
            <div className="flex flex-col gap-2">
              <p>No transcriptions available</p>
              <div className="text-sm text-gray-500">Session: {sessionId}</div>
            </div>
          )}
        </div>
      </Card>
    );
  }

  return (
    <Card className="bg-gray-900 border-gray-700">
      <div className="border-b border-gray-700 px-4 py-3 flex items-center justify-between bg-gray-900">
        <h3 className="font-medium text-gray-200">Session Transcription</h3>
        <div className="flex items-center gap-4">
          <span className="text-sm text-gray-400">Session: {sessionId}</span>
          {isRecording && (
            <div className="flex items-center gap-2 text-sm text-red-400">
              <div className="h-2 w-2 rounded-full bg-red-500 animate-pulse" />
              Recording
            </div>
          )}
        </div>
      </div>

      <div 
        ref={containerRef} 
        className="max-h-[500px] overflow-y-auto p-4 space-y-4 bg-gray-900"
      >
        {transcriptions.map((transcription, index) => (
          <div 
            key={`${transcription.timestamp}-${index}`}
            className="flex gap-3 group hover:bg-gray-800 p-2 rounded-lg transition-colors"
          >
            <div className="flex items-center gap-2 text-sm text-gray-400">
              <Clock className="w-4 h-4" />
              <span>{formatTimestamp(transcription.timestamp)}</span>
            </div>
            <p className="flex-1 text-gray-300 whitespace-pre-wrap">
              {transcription.text}
            </p>
          </div>
        ))}
      </div>
    </Card>
  );
}

================
File: src/components/ui/button.tsx
================
// src/components/ui/button.tsx
import React from 'react';

interface ButtonProps extends React.ButtonHTMLAttributes<HTMLButtonElement> {
  children: React.ReactNode;
  variant?: 'default' | 'destructive' | 'primary' | 'secondary' | 'danger';
  size?: 'default' | 'lg';
  icon?: React.ReactNode;
}

export function Button({ 
  children, 
  className = '', 
  variant = 'default',
  size = 'default',
  icon,
  ...props 
}: ButtonProps) {
  const baseStyles = 'inline-flex items-center justify-center rounded-md font-medium transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-offset-2 disabled:opacity-50 disabled:pointer-events-none';
  
  const variantStyles = {
    default: 'bg-blue-600 text-white hover:bg-blue-700',
    destructive: 'bg-red-600 text-white hover:bg-red-700',
    primary: 'bg-indigo-600 text-white hover:bg-indigo-700',
    secondary: 'bg-gray-600 text-white hover:bg-gray-700',
    danger: 'bg-yellow-600 text-black hover:bg-yellow-700'
  };
  
  const sizeStyles = {
    default: 'h-10 px-4 py-2',
    lg: 'h-12 px-6 py-3'
  };

  return (
    <button 
      className={`${baseStyles} ${variantStyles[variant]} ${sizeStyles[size]} ${className}`}
      {...props}
    >
      {icon && <span className="mr-2">{icon}</span>}
      {children}
    </button>
  );
}

export default Button;

================
File: src/components/ui/card.tsx
================
// src/components/ui/card.tsx
import React from 'react';

interface CardProps extends React.HTMLAttributes<HTMLDivElement> {
  children: React.ReactNode;
  className?: string;
}

export function Card({ children, className = '', ...props }: CardProps) {
  return (
    <div 
      className={`bg-white rounded-lg border shadow-sm ${className}`}
      {...props}
    >
      {children}
    </div>
  );
}

interface CardContentProps {
  children: React.ReactNode;
  className?: string;
}

export function CardContent({ children, className = '' }: CardContentProps) {
  return (
    <div className={`p-6 ${className}`}>
      {children}
    </div>
  );
}

export default Card;

================
File: src/components/ImageDisplay.tsx
================
'use client';

import React, { useEffect } from 'react';
import Image from 'next/image';
import { ChevronLeft, ChevronRight, Expand, Minimize } from 'lucide-react';
import { useImageNavigation } from '@/lib/hooks/useImageNavigation';

interface ImageDisplayProps {
  sessionId: string;
  isFullscreen?: boolean;
  onToggleFullscreen?: () => void;
  isRecording?: boolean;
  onImageChange?: (imageUrl: string | null) => void;
}

export default function ImageDisplay({
  sessionId,
  isFullscreen = false,
  onToggleFullscreen,
  isRecording = false,
  onImageChange
}: ImageDisplayProps) {
  const {
    currentImage,
    hasNextImage,
    hasPreviousImage,
    goToNextImage,
    goToPreviousImage,
    loading,
    totalImages,
    currentImageIndex
  } = useImageNavigation(sessionId, onImageChange);

  // Add keyboard navigation
  useEffect(() => {
    const handleKeydown = (event: KeyboardEvent) => {
      if (isRecording) return;

      switch (event.key) {
        case 'ArrowLeft':
          if (hasPreviousImage) {
            goToPreviousImage();
          }
          break;
        case 'ArrowRight':
          if (hasNextImage) {
            goToNextImage();
          }
          break;
        case 'Enter':
          if (onToggleFullscreen) {
            onToggleFullscreen();
          }
          break;
        default:
          break;
      }
    };

    window.addEventListener('keydown', handleKeydown);
    return () => {
      window.removeEventListener('keydown', handleKeydown);
    };
  }, [goToNextImage, goToPreviousImage, hasNextImage, hasPreviousImage, isRecording, onToggleFullscreen]);

  if (!currentImage) {
    return (
      <div className="flex h-full items-center justify-center bg-gray-900 rounded-lg">
        <div className={`text-gray-300 ${loading ? 'animate-pulse' : ''}`}>
          {loading ? 'Generating scene...' : 'No images available'}
        </div>
      </div>
    );
  }

  return (
    <div className="relative h-full w-full p-4">
      <div className="relative h-full w-full bg-gray-900 rounded-lg overflow-hidden">
        {/* Navigation controls - only hide during recording */}
        {!isRecording && (
          <>
            {hasPreviousImage && (
              <button
                onClick={goToPreviousImage}
                className="absolute left-4 top-1/2 -translate-y-1/2 p-2 rounded-full bg-gray-900/50 hover:bg-gray-900/75 text-white transition-all z-10"
              >
                <ChevronLeft className="h-6 w-6" />
              </button>
            )}
            
            {hasNextImage && (
              <button
                onClick={goToNextImage}
                className="absolute right-4 top-1/2 -translate-y-1/2 p-2 rounded-full bg-gray-900/50 hover:bg-gray-900/75 text-white transition-all z-10"
              >
                <ChevronRight className="h-6 w-6" />
              </button>
            )}
          </>
        )}

        {/* Fullscreen toggle */}
        {onToggleFullscreen && (
          <button
            onClick={onToggleFullscreen}
            className="absolute top-4 right-4 p-2 rounded-lg bg-gray-900/50 hover:bg-gray-900/75 text-white transition-all z-10"
          >
            {isFullscreen ? (
              <Minimize className="h-5 w-5" />
            ) : (
              <Expand className="h-5 w-5" />
            )}
          </button>
        )}

        {/* Image */}
        <div className={`relative w-full h-full transition-all duration-300 ${
          isFullscreen ? "scale-100" : "scale-95 hover:scale-100"
        }`}>
          <Image
            src={`http://localhost:3001${currentImage}`}
            alt="Generated scene"
            fill
            className="object-contain p-4"
            priority
            onError={(e) => {
              console.error('Error loading image:', e);
            }}
          />
        </div>

        {/* Image count indicator */}
        {totalImages > 1 && (
          <div className="absolute bottom-4 left-1/2 -translate-x-1/2 bg-gray-900/50 text-white px-3 py-1 rounded-full text-sm">
            {currentImageIndex + 1} / {totalImages}
          </div>
        )}
      </div>
    </div>
  );
}

================
File: src/components/RecordingProgress.tsx
================
import React, { useEffect, useState } from 'react';
import { ArrowRight } from 'lucide-react';

const RecordingProgress = () => {
  const [progress, setProgress] = useState(0);
  const chunkDuration = 10000; // 10 seconds in ms

  useEffect(() => {
    const startTime = Date.now();
    
    const interval = setInterval(() => {
      const elapsed = Date.now() - startTime;
      const newProgress = (elapsed % chunkDuration) / chunkDuration * 100;
      setProgress(newProgress);
    }, 100);

    return () => clearInterval(interval);
  }, []);

  return (
    <div className="fixed bottom-0 left-0 right-0 p-4 pointer-events-none">
      <div className="max-w-md mx-auto bg-gray-800 rounded-lg p-4 shadow-lg">
        <div className="space-y-2">
          <div className="flex justify-between text-sm text-gray-300">
            <span>Recording chunk progress</span>
            <span>{Math.round(progress)}%</span>
          </div>
          <div className="h-2 bg-gray-700 rounded-full overflow-hidden">
            <div 
              className="h-full bg-green-500 transition-all duration-300"
              style={{ width: `${progress}%` }}
            />
          </div>
          <div className="flex items-center justify-between text-xs text-gray-400">
            <span>Processing audio in {chunkDuration/1000}-second chunks</span>
          </div>
          <div className="flex items-center justify-center gap-2 text-sm text-gray-300">
            <span>Images will appear after recording on the right</span>
            <ArrowRight className="w-4 h-4 animate-pulse" />
          </div>
        </div>
      </div>
    </div>
  );
};

export default RecordingProgress;

================
File: src/components/WelcomeOverlay.tsx
================
import React from 'react';
import { Card } from '@/components/ui/card';

const WelcomeOverlay = () => {
  const examples = [
    "a castle made of cotton candy",
    "a bunny birthday party",
    "a goblin chases an elf down a dark forest path"
  ];

  return (
    <div className="absolute inset-0 flex items-center justify-center bg-gray-900/80 z-50 p-4">
      <Card className="max-w-2xl w-full bg-[#0f1524] border border-[#1a2031]"> {/* Updated dark background */}
        <div className="p-8 space-y-10">
          <h1 className="text-4xl font-obra text-gray-100 text-center mb-8">
            Welcome to Mythra
          </h1>
          
          <div className="space-y-10">
            {/* Start Section */}
            <div className="space-y-2">
              <div className="flex items-baseline gap-4">
                <span className="text-3xl font-obra font-bold text-[#22c55e]">1.</span> {/* Green number */}
                <h2 className="text-3xl font-obra font-bold text-gray-100">Start</h2>
              </div>
              <p className="text-gray-400 ml-12 font-obra">
                Press &quot;start/stop&quot; button
              </p>
            </div>

            {/* Speak Section */}
            <div className="space-y-2">
              <div className="flex items-baseline gap-4">
                <span className="text-3xl font-obra font-bold text-[#3b82f6]">2.</span> {/* Blue number */}
                <h2 className="text-3xl font-obra font-bold text-gray-100">Speak</h2>
              </div>
              <div className="ml-12">
                <p className="text-gray-400 mb-2 font-obra">Example:</p>
                <div className="space-y-2">
                  {examples.map((example, idx) => (
                    <p key={idx} className="text-gray-400 italic font-obra">
                      &quot;{example}&quot;
                    </p>
                  ))}
                </div>
              </div>
            </div>

            {/* Print Section */}
            <div className="space-y-2">
              <div className="flex items-baseline gap-4">
                <span className="text-3xl font-obra font-bold text-[#a855f7]">3.</span> {/* Purple number */}
                <h2 className="text-3xl font-obra font-bold text-gray-100">Print</h2>
              </div>
              <p className="text-gray-400 ml-12 font-obra">
                Press &quot;start/stop&quot; and then &quot;print&quot;
              </p>
            </div>
          </div>
        </div>
      </Card>
    </div>
  );
};

export default WelcomeOverlay;

================
File: src/lib/constants/paths.js
================
// src/lib/constants/paths.js
import { fileURLToPath } from 'url';
import { dirname, join } from 'path';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

// Get root project directory (3 levels up from lib/constants)
const PROJECT_ROOT = join(__dirname, '../../..');

export const PATHS = {
  AUDIO_DIR: join(PROJECT_ROOT, 'audio-chunks'),
  RECORDINGS_DIR: join(PROJECT_ROOT, 'recordings'),
  METADATA_DIR: join(PROJECT_ROOT, 'metadata'),
  IMAGES_DIR: join(PROJECT_ROOT, 'generated_images')
};

================
File: src/lib/hooks/useAudioRecorder.ts
================
// src/lib/hooks/useAudioRecorder.ts
'use client';

import { useState, useCallback, useRef, useEffect } from 'react';
import { 
  SessionMetadata, 
  WebSocketMessage, 
  TranscriptionData, 
  SessionStatus, 
  WebSocketPayload,
  defaultSessionData,
  AudioRecorderHook,
  WebSocketMessageType,
} from '../types/audio';


const createWebSocketMessage = (
  type: WebSocketMessageType,
  payload: WebSocketPayload,
  sessionId: string
): WebSocketMessage => ({
  type,
  payload,
  sessionId,
  timestamp: Date.now()
});

export function useAudioRecorder(): AudioRecorderHook {
  const [isRecording, setIsRecording] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [isConnected, setIsConnected] = useState(false);
  const [sessionData, setSessionData] = useState<SessionMetadata | null>(null);
  const [transcriptions, setTranscriptions] = useState<TranscriptionData[]>([]);
  const [sessionActive, setSessionActive] = useState(false);

  const socketRef = useRef<WebSocket | null>(null);
  const reconnectAttempts = useRef(0);
  const maxReconnectAttempts = 5;
  const lastMessageRef = useRef<string | null>(null);


  useEffect(() => {
    console.log('Debug - transcriptions state updated:', {
      count: transcriptions.length,
      transcriptions,
      isRecording,
      sessionActive
    });
  }, [transcriptions, isRecording, sessionActive]);

  useEffect(() => {
    const interval = setInterval(() => {
      if (isConnected) {
        console.log('Debug - Connection Status:', {
          isConnected,
          socketState: socketRef.current?.readyState,
          transcriptionCount: transcriptions.length,
          sessionActive,
          isRecording,
          sessionId: sessionData?.sessionId
        });
      }
    }, 5000);
  
    return () => clearInterval(interval);
  }, [isConnected, transcriptions.length, sessionActive, isRecording, sessionData]);

  const sendWebSocketMessage = useCallback((message: WebSocketMessage) => {
    if (socketRef.current?.readyState === WebSocket.OPEN) {
      socketRef.current.send(JSON.stringify(message));
    } else {
      console.warn('WebSocket is not connected');
    }
  }, []);

  const connectWebSocket = useCallback(() => {
    // Early return if already connected
    if (socketRef.current?.readyState === WebSocket.OPEN) {
      console.log('WebSocket already connected, skipping connection attempt');
      return;
    }
  
    // Close existing connection if any
    if (socketRef.current) {
      console.log('Closing existing WebSocket connection');
      socketRef.current.close();
    }
  
    console.log('Initializing WebSocket connection...', process.env.NEXT_PUBLIC_WS_URL);
    const ws = new WebSocket(process.env.NEXT_PUBLIC_WS_URL || 'ws://localhost:8080');
  
    ws.onopen = () => {
      console.log('WebSocket connected successfully');
      setIsConnected(true);
      reconnectAttempts.current = 0;
      setError(null);
    };
  
    ws.onclose = (event) => {
      console.log('Debug - WebSocket disconnected:', {
        code: event.code,
        reason: event.reason,
        wasClean: event.wasClean
      });
      setIsConnected(false);
      
      if (reconnectAttempts.current < maxReconnectAttempts) {
        console.log(`Debug - Attempting to reconnect (${reconnectAttempts.current + 1}/${maxReconnectAttempts})`);
        reconnectAttempts.current++;
        // Use fixed delay instead of exponential
        setTimeout(connectWebSocket, 2000);
      }
    };
  
    ws.onerror = (err) => {
      console.warn('WebSocket error:', err);
      
      if (!isConnected) {
        console.warn('Suppressing WebSocket connection error');
        return;
      }
      
      setError('Failed to connect to server. Is the server running?');
    };
  
    ws.onmessage = async (event) => {
      try {
        const message = JSON.parse(event.data) as WebSocketMessage;
        console.log('Debug - Received WebSocket message:', message);
    
        const messageKey = `${message.type}-${message.timestamp}`;
        if (lastMessageRef.current === messageKey) {
          console.log('Debug - Duplicate message detected, skipping');
          return;
        }
        lastMessageRef.current = messageKey;
    
        switch (message.type) {
          case 'transcription': {
            console.log('Debug - Processing transcription message:', message);
          
            if (message.payload.transcription) {
              const transcription = message.payload.transcription;
              
              setTranscriptions(prev => {
                const updated = [...(prev || []), transcription];
                console.log('Transcriptions updated:', {
                  previousCount: prev?.length || 0,
                  newCount: updated.length,
                  latest: transcription.text
                });
                return updated;
              });
            }
            break;
          }
          case 'status': {
            console.log('Debug - Processing status update:', {
              newStatus: message.payload.status,
              currentStatus: sessionData?.status,
              isRecording,
              sessionActive
            });
    
            setSessionData((prev) => ({
              ...(prev || defaultSessionData),
              sessionId: message.payload.sessionId || prev?.sessionId || '',
              status: message.payload.status as SessionStatus,
              lastUpdate: Date.now()
            }));
          
            if (message.payload.status === 'recording' && sessionActive) {
              console.log('Debug - Setting recording to true');
              setIsRecording(true);
            } else if (['completed', 'failed', 'stopped'].includes(message.payload.status || '')) {
              console.log('Debug - Setting recording to false');
              setIsRecording(false);
            }
            break;
          }
          
          case 'session_ended': {
            setIsRecording(false);
            setSessionActive(false);
            setTranscriptions([]);
            setSessionData(null);
            break;
          }
        }
      } catch (err) {
        console.error('Error processing message:', err);
      }
    };
  
    socketRef.current = ws;
  }, []);

  useEffect(() => {
    connectWebSocket();
    return () => {
      socketRef.current?.close();
    };
  }, [connectWebSocket]);

  const startSession = useCallback(async (providedSessionId?: string) => {
    try {
      if (!isConnected) {
        throw new Error('WebSocket is not connected');
      }
  
      const newSessionId = 
        providedSessionId || `session-${Date.now()}-${Math.random().toString(36).slice(2, 9)}`;
  
      const message = {
        type: 'command',
        payload: { action: 'start', sessionId: newSessionId },
        sessionId: newSessionId,
      };
  
      if (socketRef.current?.readyState === WebSocket.OPEN) {
        socketRef.current.send(JSON.stringify(message));
        setSessionActive(true);  // Changed this to true
        setTranscriptions([]);
        setSessionData({
          ...defaultSessionData,
          sessionId: newSessionId,
          startTime: Date.now(),
          status: 'ready',
        });
        return newSessionId;
      } else {
        throw new Error('WebSocket is not in an open state');
      }
    } catch (err) {
      console.warn('Error starting session:', err);
      setError(err instanceof Error ? err.message : 'Failed to start session');
      throw err;
    }
  }, [isConnected]);  
  

  const startRecording = useCallback(async () => {
    try {
      setError(null);
  
      if (!isConnected) {
        throw new Error('Not connected to server');
      }
  
      let currentSessionId = sessionData?.sessionId;
  
      // If no session is active, start a new session
      if (!sessionActive) {
        currentSessionId = await startSession();
      }
  
      if (!currentSessionId) {
        throw new Error('Invalid session state');
      }
  
      // Prepare the WebSocket message
      const message: WebSocketMessage = {
        type: 'command', // Valid type from WebSocketMessageType
        payload: {
          action: 'startRecording', // Valid action from WebSocketPayload
          sessionId: currentSessionId,
        },
        sessionId: currentSessionId,
        timestamp: Date.now(), // Add a timestamp for the message
      };
  
      sendWebSocketMessage(message); // Send the message
      console.log('Frontend: Sent startRecording message to server');
  
      setIsRecording(true); // Update recording state
      console.log('isRecording updated to true:', isRecording);
    } catch (err) {
      console.warn('Error starting recording:', err);
      setError(err instanceof Error ? err.message : 'Failed to start recording');
    }
  }, [isConnected, isRecording, sessionActive, sessionData, startSession, sendWebSocketMessage]);
  
  const stopRecording = useCallback(() => {
    try {
      console.log('Frontend: Initiating stop recording...');
  
      if (!sessionData?.sessionId) {
        throw new Error('No active session found for stop recording');
      }
  
      // Prepare the WebSocket message
      const message: WebSocketMessage = {
        type: 'command', // Valid type from WebSocketMessageType
        payload: {
          action: 'stop', // Valid action from WebSocketPayload
          sessionId: sessionData.sessionId,
        },
        sessionId: sessionData.sessionId,
        timestamp: Date.now(), // Add a timestamp for the message
      };
  
      sendWebSocketMessage(message); // Send the message
      console.log('Frontend: Sent stop command for session:', sessionData.sessionId);
  
      setIsRecording(false); // Update recording state
    } catch (err) {
      console.warn('Error stopping recording:', err);
      setError(err instanceof Error ? err.message : 'Failed to stop recording');
    }
  }, [sessionData, sendWebSocketMessage]);
  
  
  
  

  const endSession = useCallback(() => {
    if (isRecording) {
      stopRecording();
    }
    if (sessionData?.sessionId) {
      const message = createWebSocketMessage(
        'command',
        { action: 'end', sessionId: sessionData.sessionId },
        sessionData.sessionId
      );
      sendWebSocketMessage(message);
    }
    setSessionActive(false);
    setTranscriptions([]);
    setSessionData(null);
  }, [isRecording, stopRecording, sessionData, sendWebSocketMessage]);

  return {
    isRecording,
    startRecording,
    stopRecording,
    endSession,
    error,
    audioLevel: 0,
    isConnected,
    sessionData,
    transcriptions,
    sessionActive,
    sessionId: sessionData?.sessionId ?? null,
    startSession,
    sendWebSocketMessage,
  };
}

================
File: src/lib/hooks/useImageNavigation.ts
================
'use client';

import { useState, useEffect } from 'react';
import { normalizeSessionId } from '../utils/session';

interface ImageNavigationState {
  currentImageIndex: number;
  totalImages: number;
  images: string[];
}

interface ImageApiResponse {
  images?: string[];
  imagePath?: string;
  error?: string;
}

export interface UseImageNavigation {
  currentImage: string | null;
  currentImageIndex: number;
  totalImages: number;
  hasNextImage: boolean;
  hasPreviousImage: boolean;
  goToNextImage: () => void;
  goToPreviousImage: () => void;
  loading: boolean;
}

export function useImageNavigation(
  sessionId: string,
  onImageChange?: (imageUrl: string | null) => void
): UseImageNavigation {
  const [state, setState] = useState<ImageNavigationState>({
    currentImageIndex: 0,
    totalImages: 0,
    images: [],
  });
  const [loading, setLoading] = useState(false);

  // Add this effect to call onImageChange whenever the current image changes
  useEffect(() => {
    if (onImageChange) {
      onImageChange(state.images[state.currentImageIndex] || null);
    }
  }, [state.images, state.currentImageIndex, onImageChange]);

  useEffect(() => {
    if (!sessionId) {
      console.log('No session ID provided');
      return;
    }

    const fetchImages = async () => {
      try {
        setLoading(true);
        const normalizedId = normalizeSessionId(sessionId);
        
        // First try to get the latest image
        const latestResponse = await fetch(`http://localhost:3001/api/images/latest/${sessionId}`);
        
        if (latestResponse.ok) {
          const latestData = await latestResponse.json() as ImageApiResponse;
          
          if (latestData.imagePath) {
            setState(prev => {
              if (prev.images.length > 0) {
                if (!prev.images.includes(latestData.imagePath!)) {
                  const newImages = [...prev.images, latestData.imagePath!];
                  return {
                    images: newImages,
                    totalImages: newImages.length,
                    currentImageIndex: newImages.length - 1
                  };
                }
                return prev;
              }
              return {
                images: [latestData.imagePath!],
                totalImages: 1,
                currentImageIndex: 0
              };
            });
          }
        } else {
          const allResponse = await fetch('http://localhost:3001/api/images');
          
          if (allResponse.ok) {
            const data = await allResponse.json() as ImageApiResponse;
            if (data.images && data.images.length > 0) {
              const sessionImages = data.images
                .filter(img => img.includes(normalizedId))
                .sort((a, b) => {
                  const getTimestamp = (filename: string) => {
                    const match = filename.match(/-(\d+)\.png$/);
                    return match ? parseInt(match[1]) : 0;
                  };
                  return getTimestamp(a) - getTimestamp(b);
                });

              if (sessionImages.length > 0) {
                setState({
                  images: sessionImages,
                  totalImages: sessionImages.length,
                  currentImageIndex: sessionImages.length - 1
                });
              }
            }
          }
        }
      } catch (error) {
        console.error('Error fetching images:', error);
      } finally {
        setLoading(false);
      }
    };

    fetchImages();
    const interval = setInterval(fetchImages, 3000);
    return () => clearInterval(interval);
  }, [sessionId]);

  const goToNextImage = () => {
    console.log('Navigating to next image from:', state.currentImageIndex);
    setState(prev => {
      const newIndex = Math.min(prev.currentImageIndex + 1, prev.images.length - 1);
      console.log('New image index:', newIndex, 'URL:', prev.images[newIndex]);
      return {
        ...prev,
        currentImageIndex: newIndex
      };
    });
  };
  
  const goToPreviousImage = () => {
    console.log('Navigating to previous image from:', state.currentImageIndex);
    setState(prev => {
      const newIndex = Math.max(prev.currentImageIndex - 1, 0);
      console.log('New image index:', newIndex, 'URL:', prev.images[newIndex]);
      return {
        ...prev,
        currentImageIndex: newIndex
      };
    });
  };

  const hasNextImage = state.images.length > 1 && state.currentImageIndex < state.images.length - 1;
  const hasPreviousImage = state.images.length > 1 && state.currentImageIndex > 0;

  return {
    currentImage: state.images[state.currentImageIndex] || null,
    currentImageIndex: state.currentImageIndex,
    totalImages: state.images.length,
    hasNextImage,
    hasPreviousImage,
    goToNextImage,
    goToPreviousImage,
    loading
  };
}

================
File: src/lib/services/audioSession.js
================
// src/lib/services/audioSession.js
import { PATHS } from '../constants/paths.js';
import fs from 'fs/promises';
import { mkdirSync } from 'fs';
import { join } from 'path';
import { RecordingService } from './recordingService.js';

const { AUDIO_DIR, RECORDINGS_DIR, METADATA_DIR } = PATHS;

// Ensure necessary directories exist
mkdirSync(AUDIO_DIR, { recursive: true });
mkdirSync(RECORDINGS_DIR, { recursive: true });
mkdirSync(METADATA_DIR, { recursive: true });

export class AudioSession {
  constructor(sessionId, transcriptionService) {
    this.sessionId = sessionId;
    this.startTime = Date.now();
    this.chunks = new Map();
    this.status = 'ready';
    this.transcriptionService = transcriptionService;
    this.socket = null;
    this._processedChunks = new Set();
    this.metadata = {
      totalDuration: 0,
      totalSize: 0,
      lastChunkId: -1,
      checksums: new Set(),
      transcriptions: []
    };
    
    this.recordingService = new RecordingService();

    // Add event handlers for transcription service
    this.transcriptionService.on('transcription', (transcriptionData) => {
      console.log('AudioSession: Received transcription event:', transcriptionData);
      this.handleTranscriptionEvent(transcriptionData);
    });

    this.transcriptionService.on('error', (error) => {
      console.error('AudioSession: Transcription error:', error);
      this.sendError(`Transcription error: ${error.message}`);
    });
  }

  // Add new method to handle transcription events
  handleTranscriptionEvent(transcriptionData) {
    try {
      if (!this.socket) {
        console.warn('AudioSession: No socket connection available for sending transcription');
        return;
      }

      console.log('AudioSession: Sending transcription event to client:', transcriptionData);
      
      const message = {
        type: 'transcription',
        payload: {
          transcription: transcriptionData
        },
        sessionId: this.sessionId,
        timestamp: Date.now()
      };

      this.socket.send(JSON.stringify(message));
      console.log('AudioSession: Transcription event sent to client successfully');
    } catch (error) {
      console.error('AudioSession: Error sending transcription to client:', error);
    }
  }

  // Your existing methods remain unchanged
  setSocket(socket) {
    this.socket = socket;
  }

  sendStatus(message) {
    if (this.socket) {
      this.socket.send(JSON.stringify({
        type: 'status',
        payload: { message, status: this.status },
        timestamp: Date.now(),
        sessionId: this.sessionId
      }));
    }
  }

  sendError(message) {
    if (this.socket) {
      this.socket.send(JSON.stringify({
        type: 'error',
        payload: { message },
        timestamp: Date.now(),
        sessionId: this.sessionId
      }));
    }
  }

  async startRecording() {
    try {
      this.status = 'recording';
      this.sendStatus('Starting recording...');
  
      this.recordingService.startRecording(this.sessionId, async (sessionId, chunkId, filePath) => {
        try {
          const data = await fs.readFile(filePath);
          await this.processChunk(chunkId, data);
          
          if (this.socket) {
            this.socket.send(JSON.stringify({
              type: 'ack',
              payload: { chunkId },
              sessionId: this.sessionId,
              timestamp: Date.now()
            }));
          }
        } catch (error) {
          console.error(`Error processing chunk ${chunkId}:`, error);
          this.sendError(`Error processing chunk ${chunkId}: ${error.message}`);
        }
      });
  
    } catch (error) {
      console.error('Error starting recording:', error);
      this.sendError(`Failed to start recording: ${error.message}`);
      throw error;
    }
  }

  async stopRecording() {
    try {
      console.log(`[AudioSession] Stopping recording for session ${this.sessionId}`);
      
      if (this.recordingService.isRecording(this.sessionId)) {
        // Stop the recording service
        await this.recordingService.stopRecording(this.sessionId);
        console.log('[AudioSession] RecordingService stopped');
        
        // Update status
        this.status = 'stopped';
        
        // Send status update to client
        if (this.socket) {
          console.log('[AudioSession] Sending stopped status to client');
          this.socket.send(JSON.stringify({
            type: 'status',
            payload: {
              status: 'stopped',
              sessionId: this.sessionId
            },
            sessionId: this.sessionId,
            timestamp: Date.now()
          }));
        }
      }
    } catch (error) {
      console.error('Error stopping recording:', error);
      this.sendError(`Failed to stop recording: ${error.message}`);
      throw error;
    }
  }

  async processChunk(chunkId, data) {
    try {
      console.log(`[AudioSession] Processing chunk ${chunkId} for session ${this.sessionId}`);
      
      // Create the chunk path first
      const chunkPath = join(AUDIO_DIR, this.sessionId, `chunk-${chunkId}.webm`);
      
      // Validate chunk not already processed
      const chunkKey = `${this.sessionId}-${chunkId}`;
      if (this._processedChunks.has(chunkKey)) {
        console.log(`[AudioSession] Chunk ${chunkId} already processed, skipping`);
        return null;
      }
  
      // Verify the file exists and has content
      try {
        const stats = await fs.stat(chunkPath);
        console.log(`[AudioSession] Chunk file exists with size: ${stats.size} bytes`);
        if (stats.size === 0) {
          throw new Error('Chunk file is empty');
        }
      } catch (err) {
        console.error(`[AudioSession] Failed to read chunk file:`, err);
        return null;
      }
  
      // Process chunk and get transcription
      const chunk = {
        path: chunkPath,
        size: data.length,
        timestamp: Date.now(),
      };
      
      console.log(`[AudioSession] Starting transcription for chunk ${chunkId}`);
      const transcription = await this.transcribeChunk(chunk, chunkId);
      
      if (transcription) {
        // Update transcription file
        const transcriptionPath = join(METADATA_DIR, `${this.sessionId}-transcription.json`);
        let transcriptionData;
        
        try {
          const existingData = await fs.readFile(transcriptionPath, 'utf-8');
          transcriptionData = JSON.parse(existingData);
        } catch {
          transcriptionData = {
            sessionId: this.sessionId,
            lastUpdated: Date.now(),
            transcriptions: []
          };
        }
  
        // Add new transcription
        transcriptionData.transcriptions.push(transcription);
        transcriptionData.lastUpdated = Date.now();
        
        // Save file only - remove WebSocket send since it's handled by the event
        await fs.writeFile(transcriptionPath, JSON.stringify(transcriptionData, null, 2));
        
        // Remove this entire block since it's duplicate
        // if (this.socket) {
        //   console.log(`[AudioSession] Sending transcription to client:`, transcription);
        //   this.socket.send(JSON.stringify({
        //     type: 'transcription',
        //     payload: { transcription },
        //     sessionId: this.sessionId,
        //     timestamp: Date.now()
        //   }));
        // }
      }
  
      // Mark chunk as processed
      this._processedChunks.add(chunkKey);
      return chunk;
      
    } catch (error) {
      console.error(`[AudioSession] Error processing chunk ${chunkId}:`, error);
      throw error;
    }
  }

  async saveMetadata() {
    const metadataPath = join(METADATA_DIR, `${this.sessionId}.json`);
    await fs.writeFile(metadataPath, JSON.stringify(this.metadata, null, 2));
  }

  async transcribeChunk(chunk, chunkId) {
    try {
      console.log(`Transcribing chunk ${chunkId} at path: ${chunk.path}`);
      
      // Wait for the file to be fully written
      await new Promise(resolve => setTimeout(resolve, 100));
      
      const transcription = await this.transcriptionService.transcribeFile(chunk.path, this.sessionId);
      
      return transcription;
    } catch (error) {
      console.error(`Error transcribing chunk ${chunkId}:`, error);
      return null;
    }
  }

  async finalize() {
    try {
      this.status = 'transcribing';
      this.sendStatus('Transcribing audio chunks...');

      const transcriptions = [];

      // Process each chunk's transcription
      for (let i = 0; i <= this.metadata.lastChunkId; i++) {
        const chunk = this.chunks.get(i);
        if (!chunk) {
          console.warn(`Missing chunk ${i} during finalization`);
          continue;
        }

        const transcription = await this.transcribeChunk(chunk, i);
        if (transcription) {
          transcriptions.push(transcription);
        }
      }

      // Save transcriptions file
      const transcriptionPath = join(METADATA_DIR, `${this.sessionId}-transcription.json`);
      const transcriptionData = {
        sessionId: this.sessionId,
        lastUpdated: Date.now(),
        transcriptions
      };

      await fs.writeFile(transcriptionPath, JSON.stringify(transcriptionData, null, 2));
      
      this.metadata.transcriptions = transcriptions;
      this.status = 'completed';
      await this.saveMetadata();

      this.sendStatus('Transcription completed');
      
      return { 
        sessionId: this.sessionId,
        transcriptions
      };
    } catch (error) {
      this.status = 'error';
      this.sendError(`Transcription failed: ${error.message}`);
      console.error('Error in finalize:', error);
      throw error;
    }
  }
}

================
File: src/lib/services/generateImageFlux.js
================
import axios from 'axios';
import fs from 'fs';
import path from 'path';
import WebSocket from 'ws';
import { normalizeSessionId } from '../utils/session.js';


const rootDir = process.cwd();
const API_KEY = process.env.BFL_API_KEY;

export const generateImageFlux = async (prompt, sessionId, width = 1024, height = 768) => {

  const normalizedSessionId = normalizeSessionId(sessionId);
  console.log('Image generation session ID:', {
    original: sessionId,
    normalized: normalizedSessionId
  });

  const promptPrefix =
    "High-fantasy, photorealistic illustration for a DND campaign. The scene should evoke epic adventure, rich in detail, dramatic lighting, and set in a magical world. The story is about the following: ";
  const fullPrompt = `${promptPrefix}${prompt}`;

  try {
    // Normalize the session ID
    const normalizedSessionId = normalizeSessionId(sessionId);
    console.log('Session ID handling:', {
      original: sessionId,
      normalized: normalizedSessionId
    });

    // Ensure the images directory exists
    const dirPath = path.join(rootDir, 'generated_images');
    console.log('Checking images directory:', dirPath);
    if (!fs.existsSync(dirPath)) {
      console.log('Creating images directory...');
      fs.mkdirSync(dirPath, { recursive: true });
    }

    // Step 1: Send the image generation request
    console.log('Sending image generation request...');
    const requestResponse = await axios.post(
      'https://api.bfl.ml/v1/flux-pro-1.1',
      {
        prompt: fullPrompt,
        width: width,
        height: height,
      },
      {
        headers: {
          accept: 'application/json',
          'x-key': API_KEY,
          'Content-Type': 'application/json',
        },
      }
    );

    const requestId = requestResponse.data.id;
    console.log('Request ID:', requestId);

    // Step 2: Poll for the result
    let imageUrl = null;
    let attempts = 0;
    const maxAttempts = 60;
    
    while (!imageUrl && attempts < maxAttempts) {
      console.log(`Waiting for image generation... Attempt ${attempts + 1}/${maxAttempts}`);
      await new Promise((resolve) => setTimeout(resolve, 500));

      const resultResponse = await axios.get(
        'https://api.bfl.ml/v1/get_result',
        {
          headers: {
            accept: 'application/json',
            'x-key': API_KEY,
          },
          params: {
            id: requestId,
          },
        }
      );

      if (resultResponse.data.status === 'Ready') {
        imageUrl = resultResponse.data.result.sample;
      } else {
        console.log(`Status: ${resultResponse.data.status}`);
        attempts++;
      }
    }

    if (!imageUrl) {
      throw new Error('Image generation timed out');
    }

    console.log(`Image URL: ${imageUrl}`);

    // Step 3: Download and save the image
    console.log('Downloading image...');
    const imageResponse = await axios.get(imageUrl, { responseType: 'arraybuffer' });
    
    const timestamp = Date.now();
    const filename = `${normalizedSessionId}-${timestamp}.png`;
    const imagePath = path.join(dirPath, filename);
    
    console.log(`Saving image to: ${imagePath}`);
    await fs.promises.writeFile(imagePath, imageResponse.data);
    console.log('Image saved successfully');

    // Notify WebSocket server with normalized session ID
    try {
      console.log('Notifying WebSocket server...');
      const ws = new WebSocket('ws://localhost:8080');
      
      await new Promise((resolve) => {
        ws.onopen = () => {
          const message = {
            type: 'newImage',
            sessionId: normalizedSessionId, // Use normalized ID here
            imagePath: `/api/images/${filename}`
          };
          console.log('Sending WebSocket message:', message);
          ws.send(JSON.stringify(message));
          ws.close();
          console.log('WebSocket notification sent');
          resolve();
        };
      });
    } catch (wsError) {
      console.error('Error notifying WebSocket server:', wsError);
    }

    return `/api/images/${filename}`;
  } catch (error) {
    console.error('Error during image generation:', error);
    console.error('Stack trace:', error.stack);
    return null;
  }
};

// Test function
if (import.meta.url === `file://${process.argv[1]}`) {
  (async () => {
    const testPrompt = `There is a misty forest at the foot of a mountain. There is a team of 3 characters near a cave at the foot of the mountain: A brown skinned female elf warrior with black hair wears heavy armor, a bow and quiver of arrows are slung over her back, a short scruffy dwarf with leather clothing is holding a war hammer, and a humanoid dragon who is blue in color and wears traveling clothes, their dragon-like hand rests on the hilt of their sword in its scabbard.

The brown female elf, the dwarf, and the blue humanoid dragon creep, trying not to be seen,  behind large boulders that flank the sides of the archway leading into a hall-like cavern. On the wall opposite the entrance there is an altar with glowing symbols and flame torches sparsely light the walls of the cavern. This is an action sequence where the characters are fighting a goblin.`;
    const testSessionId = `test-session-${Date.now()}`;

    try {
      const imagePath = await generateImageFlux(testPrompt, testSessionId);
      if (imagePath) {
        console.log(`Image saved successfully: ${imagePath}`);
      } else {
        console.error('Image generation failed.');
      }
    } catch (error) {
      console.error('Error during test execution:', error);
    }
  })();
}

================
File: src/lib/services/generateImagePrompt.js
================
import OpenAI from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export const generateImagePrompt = async (transcription, recentActivity, memory) => {
  const prompt = `
    You are a creative assistant tasked with generating image prompts for a DND campaign. 
    Based on the transcription, recent activity, and memory, decide if an image prompt is needed. 
    Only generate a new image if there is a significant visual scene change or important character moment.

    Rules for deciding if an image is needed:
    - Must be a new visual scene or location
    - Must involve clear visual elements that would make a compelling image
    - Should avoid generating images for simple movements or minor actions
    - Should focus on epic moments, location changes, or significant character introductions

    If no image is needed, return exactly: "No image needed"
    If an image is needed, start your response with "Generate image:"
    
    ### Inputs ###
    Transcription: "${transcription}"
    Recent Activity: "${recentActivity}"
    Memory: ${JSON.stringify(memory, null, 2)}
  `;

  try {
    console.log('Calling LLM for image prompt...');

    const response = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: prompt }],
    });

    const result = response.choices[0]?.message?.content?.trim();
    console.log('Raw LLM Response:', result);

    if (!result || result.toLowerCase() === 'no image needed') {
      console.log('LLM decided no image is needed');
      return null;
    }

    if (!result.toLowerCase().startsWith('generate image:')) {
      console.log('Unexpected LLM response format');
      return null;
    }

    // Return the cleaned prompt
    return result.replace(/^generate image:/i, '').trim();
  } catch (error) {
    console.error('Error generating image prompt:', error.message);
    return null;
  }
};

================
File: src/lib/services/llmPrompt.js
================
/**
 * Generate the LLM prompt for memory updates.
 *
 * @param {string} recentActivity - The recent transcription activity.
 * @param {object} currentMemory - The current state of memory.
 * @returns {string} - The LLM prompt.
 */
export const generateLLMPrompt = (recentActivity, currentMemory) => {
    return `
  You are a memory manager for a Dungeons & Dragons story. Here's the current transcription of recent activity:
  ---
  ${recentActivity}
  
  Current Memory:
  ${JSON.stringify(currentMemory, null, 2)}
  
  Update the memory by extracting:
  1. New characters, locations, or items introduced.
  2. Significant events or actions worth storing in long-term memory.
  Respond in JSON format with "characters", "locations", and "items".
    `;
  };

================
File: src/lib/services/memoryProcessor.js
================
export const generateMemoryUpdates = (recentActivity, currentMemory) => {
    console.log('--- LLM Memory Generation Placeholder ---');
    console.log('Recent Activity Buffer:', recentActivity);
    console.log('Current Memory:', currentMemory);
  
    // Mock output based on recent activity
    const mockUpdates = {
      characters: [
        {
          name: 'Tazghull',
          species: 'Goliath',
          class: 'Barbarian',
          itemsEquipped: ['Glowing Sword'],
        },
      ],
      locations: [
        {
          name: 'Harrowmill',
          description: 'A mysterious hooded figure was encountered.',
        },
      ],
      items: [
        {
          name: 'Glowing Sword',
          description: 'Emits a magical aura when wielded.',
        },
      ],
    };
  
    console.log('Generated Mock Updates:', mockUpdates);
    return mockUpdates;
  };

================
File: src/lib/services/memoryPrompt.js
================
export const generateMemoryPrompt = (transcription, memory, recentContext) => {
    return `
      You are managing a dynamic memory table for a Dungeons and Dragons game (DND). 
      The memory table contains characters, items, and locations. 

      You will receive:
        1. The current new line of transcription.
        2. Recent context (previous lines of transcription to give you a brief background and understanding of the current transcript)
        2. The current memory table with characters, items, and locations along with their descriptions that you will be updating.
  
      ### Your Task ###
      Based on the new transcription, recent context, and the current memory table, update the memory table to include any new or updated details that would be important to retain for visual reference only. The goal of the memory table to is to act as a reference to generate images, so please focus on visual descriptions. 
      - Do not delete or remove existing entries unless explicitly contradicted in the transcription.
      - Merge any duplicate or overlapping entries. For example, if "Bruce the warrior" and "Bruce" refer to the same character, combine their details into a single entry.
      - Include only details explicitly mentioned in the transcription.
      - Ensure the updated memory table remains logically consistent.
      - Include only the updated memory table in JSON format without additional text.

  
      ### Memory Table Before Update ###
      ${JSON.stringify(memory, null, 2)}

      ### Recent Context ###
      "${recentContext}"
  
      ### New Transcription ###
      "${transcription}"
  
      ### Updated Memory Table ###
      Provide the updated memory table as a valid JSON object.
  
      ### Examples ###
  
      Example 1:
      Current Transcription: "Bruce, a tall Goliath warrior with a glowing sword, stands on the edge of the battlefield."
      Recent Transcription: "Azghol, the dark sorcerer, watches from the shadows."
      Memory Table Before Update:
      {
        "characters": [
          { "name": "Azghol", "description": "dark sorcerer, lurks in shadows" }
        ],
        "items": [],
        "locations": []
      }
      Updated Memory Table:
      {
        "characters": [
          { "name": "Bruce", "description": "A tall Goliath warrior with a Glowing Sword" }
          { "name": "Azghol", "description": "dark sorcerer, lurks in shadows" }
        ],
        "items": [
          { "name": "Glowing Sword", "description": "A faintly glowing sword owned by Bruce." }
        ],
        "locations": []
      }
  
      Example 2:
      Transcription: "Todd, the radiant Asmere Paladin, studies an ancient map in the ruins of a long-forgotten temple."
      Memory Table Before Update:
      {
        "characters": [],
        "items": [],
        "locations": []
      }
      Updated Memory Table:
      {
        "characters": [
          { "name": "Todd", "description": "A radiant Asmere Paladin" }
        ],
        "items": [
          { "name": "Ancient Map", "description": "A weathered map, held by Todd" }
        ],
        "locations": [
          { "name": "Temple Ruins", "description": "A long-forgotten temple with ancient ruins." }
        ]
      }
  
      Example 3:
      Transcription: "A goblin ambushes Bruce and Todd as they enter a dark cave covered in moss."
      Memory Table Before Update:
      {
        "characters": [
          { "name": "Bruce", "description": "A tall Goliath warrior." },
          { "name": "Todd", "description": "A radiant Asmere Paladin." }
        ],
        "items": [
          { "name": "Glowing Sword", "description": "A faintly glowing sword owned by Bruce." }
        ],
        "locations": [
          { "name": "Temple Ruins", "description": "A long-forgotten temple with ancient ruins." }
        ]
      }
      Updated Memory Table:
      {
        "characters": [
          { "name": "Bruce", "description": "A tall Goliath warrior ambushed by a goblin in a mossy cave." },
          { "name": "Todd", "description": "A radiant Asmere Paladin ambushed by a goblin in a mossy cave." },
          { "name": "Goblin", "description": "A creature ambushing Bruce and Todd in a mossy cave." }
        ],
        "items": [
          { "name": "Glowing Sword", "description": "A faintly glowing sword owned by Bruce." }
        ],
        "locations": [
          { "name": "Temple Ruins", "description": "A long-forgotten temple with ancient ruins." },
          { "name": "Mossy Cave", "description": "A dark cave covered in moss" }
        ]
      }
  
      ### End of Examples ###
  
     ### Output:
        Provide the updated memory table as valid JSON, ensuring it includes:
        - Updated characters, items, and locations.
        - Updated processedChunks array to track handled transcription chunks.
  `;
};

================
File: src/lib/services/recordingService.js
================
// src/lib/services/recordingService.js
import fs from 'fs';
import path from 'path';
import ffmpeg from 'fluent-ffmpeg';
import ffmpegInstaller from '@ffmpeg-installer/ffmpeg';
import { PATHS } from '../constants/paths.js';


ffmpeg.setFfmpegPath(ffmpegInstaller.path);

export class RecordingService {
    constructor() {
      this.sessions = new Map();
      this.CHUNK_DURATION = 10;
      this.AUDIO_DIR = PATHS.AUDIO_DIR;
      
      console.log('RecordingService initialized with AUDIO_DIR:', this.AUDIO_DIR);
      if (!fs.existsSync(this.AUDIO_DIR)) {
        fs.mkdirSync(this.AUDIO_DIR, { recursive: true });
      }
    }
  
    startRecording(sessionId, onChunkComplete) {
      console.log(`[RecordingService] Starting/Resuming recording for session: ${sessionId}`);
      
      // Check if session exists and is in a valid state
      const existingSession = this.sessions.get(sessionId);
      if (existingSession) {
        if (existingSession.isRecording) {
          throw new Error('Session already recording');
        }
        if (existingSession.isStopping) {
          throw new Error('Session is currently stopping');
        }
      }
      
      // Create session directory if needed
      const sessionDir = path.join(this.AUDIO_DIR, sessionId);
      console.log(`[RecordingService] Session directory: ${sessionDir}`);
      
      if (!fs.existsSync(sessionDir)) {
        console.log(`[RecordingService] Creating new session directory`);
        fs.mkdirSync(sessionDir, { recursive: true });
      }
    
      // Initialize or get session data
      let sessionData = existingSession || {
        process: null,
        lastChunkId: -1,
        startTime: Date.now(),
        isRecording: false,
        isStopping: false
      };
    
      console.log(`[RecordingService] Session data:`, sessionData);
    
      if (sessionData.process) {
        console.log(`[RecordingService] Cleaning up existing process`);
        sessionData.process.kill('SIGTERM');
        sessionData.process = null;
      }
    
      sessionData.isRecording = true;
      sessionData.isStopping = false;
    
      const recordChunk = () => {
        if (!sessionData.isRecording) {
          console.log(`[RecordingService] Recording stopped for session ${sessionId}`);
          return;
        }
  
        const chunkId = sessionData.lastChunkId + 1;
        const filename = path.join(sessionDir, `chunk-${chunkId}.webm`);
        console.log(`[RecordingService] Starting chunk ${chunkId} recording to ${filename}`);
  
        const process = ffmpeg()
          .input(':0')
          .inputFormat('avfoundation')
          .audioCodec('libopus')
          .format('webm')
          .duration(this.CHUNK_DURATION)
          .on('start', (cmd) => {
            console.log(`[RecordingService] Recording chunk ${chunkId} started with command: ${cmd}`);
          })
          .on('end', () => {
            console.log(`[RecordingService] Chunk ${chunkId} recording completed`);
            
            // Verify file exists and has content
            if (fs.existsSync(filename)) {
              const stats = fs.statSync(filename);
              console.log(`[RecordingService] Chunk ${chunkId} file size: ${stats.size} bytes`);
              
              if (stats.size > 0) {
                sessionData.lastChunkId = chunkId;
                
                // Wait a moment for the file to be fully written
                setTimeout(() => {
                  onChunkComplete(sessionId, chunkId, filename);
                  
                  // Start next chunk if still recording
                  if (sessionData.isRecording) {
                    console.log(`[RecordingService] Starting next chunk`);
                    setTimeout(recordChunk, 0);
                  }
                }, 100);
              } else {
                console.error(`[RecordingService] Chunk ${chunkId} is empty`);
                if (sessionData.isRecording) {
                  recordChunk();
                }
              }
            } else {
              console.error(`[RecordingService] Chunk ${chunkId} was not created`);
              if (sessionData.isRecording) {
                recordChunk();
              }
            }
          })
          .on('error', (err) => {
            console.error(`[RecordingService] Error during recording chunk ${chunkId}:`, err.message);
            if (sessionData.isRecording) {
              recordChunk();
            }
          })
          .save(filename);
  
        sessionData.process = process;
      };
  
      this.sessions.set(sessionId, sessionData);
      recordChunk();
      
      return sessionData;
    }
  
    stopRecording(sessionId) {
      console.log(`[RecordingService] Stopping recording for session: ${sessionId}`);
      const session = this.sessions.get(sessionId);
      
      if (!session) {
        console.warn(`[RecordingService] No session found for ID: ${sessionId}`);
        return Promise.resolve();
      }
    
      session.isStopping = true;
      session.isRecording = false;
      
      return new Promise((resolve) => {
        if (session.process) {
          console.log(`[RecordingService] Killing ffmpeg process`);
          
          // Handle process termination
          session.process.on('close', () => {
            console.log(`[RecordingService] ffmpeg process terminated`);
            session.process = null;
            session.isStopping = false;
            session.lastChunkId = -1;
            this.sessions.set(sessionId, {
              ...session,
              process: null,
              isRecording: false,
              isStopping: false,
              lastChunkId: -1
            });
            resolve();
          });
    
          // Add error handler
          session.process.on('error', (err) => {
            console.log(`[RecordingService] Process error:`, err);
            session.process = null;
            session.isStopping = false;
            resolve();  // Resolve even on error
          });
    
          try {
            session.process.kill('SIGTERM');
          } catch (err) {
            console.error('[RecordingService] Error killing process:', err);
            session.process = null;
            session.isStopping = false;
            resolve();  // Resolve if kill fails
          }
        } else {
          session.isStopping = false;
          resolve();
        }
      });
    }
  
    isRecording(sessionId) {
      const session = this.sessions.get(sessionId);
      return session?.isRecording || false;
    }
  
    getSessionData(sessionId) {
      return this.sessions.get(sessionId);
    }
  }

================
File: src/lib/services/transcription.js
================
// src/lib/services/transcription.js
import fs from 'fs';
import fetch from 'node-fetch';
import FormData from 'form-data';
import { EventEmitter } from 'events';
import { OPENAI_API_KEY } from '../../env.js';

export default class TranscriptionService extends EventEmitter {
  constructor() {
    super();
    this.apiKey = OPENAI_API_KEY;
    if (!this.apiKey) {
      throw new Error('OpenAI API key is missing');
    }
  }

  async transcribeFile(filePath, sessionId) {
    try {
      console.log('TranscriptionService: Starting transcription for file:', filePath);

      // Validate file exists and has content
      const stats = await fs.promises.stat(filePath);
      console.log('File stats:', stats);
      
      if (stats.size === 0) {
        throw new Error('File is empty');
      }

      // Create form data
      const form = new FormData();
      const fileBuffer = fs.readFileSync(filePath);
      
      form.append('file', fileBuffer, {
        filename: 'audio.webm',
        contentType: 'audio/webm',
      });

      form.append('model', 'whisper-1');
      form.append('language', 'en');

      console.log('TranscriptionService: Sending request to OpenAI...');

      const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          Authorization: `Bearer ${this.apiKey}`,
          ...form.getHeaders(),
        },
        body: form,
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error('TranscriptionService: Error response:', errorText);
        throw new Error(`Transcription failed: ${response.status} ${response.statusText} - ${errorText}`);
      }

      const result = await response.json();
      console.log('TranscriptionService: Transcription successful:', result);

      const transcriptionData = {
        text: result.text,
        timestamp: Date.now(),
        sessionId,
      };

      // Emit the transcription event
      console.log('TranscriptionService: Emitting transcription event:', transcriptionData);
      this.emit('transcription', transcriptionData);

      return transcriptionData;
    } catch (error) {
      console.error('TranscriptionService: Error during transcription:', error);
      this.emit('error', error);
      throw error;
    }
  }

  // Helper method to validate audio file
  async validateAudioFile(filePath) {
    try {
      const stats = await fs.promises.stat(filePath);
      const fileSizeInMB = stats.size / (1024 * 1024);

      console.log(`File size: ${fileSizeInMB.toFixed(2)} MB`);

      if (fileSizeInMB > 25) {
        throw new Error('Audio file is too large. Maximum size is 25MB.');
      }

      return true;
    } catch (error) {
      console.error('File validation error:', error);
      throw error;
    }
  }
}

================
File: src/lib/test/test-transcription.js
================
// src/lib/test/test-transcription.js
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import { dirname } from 'path';
import TranscriptionService from '../services/transcription.js';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

async function testTranscription() {
  try {
    console.log('Starting transcription test...');
    
    const transcriptionService = new TranscriptionService();
    
    // Log the API key (first few characters only)
    const apiKeyPreview = transcriptionService.apiKey ? 
      `${transcriptionService.apiKey.substring(0, 5)}...` : 
      'not found';
    console.log('API Key preview:', apiKeyPreview);

    // Test file path
    const testFilePath = path.join(__dirname, '../../../recordings/test-audio.webm');

    // Check if file exists and log details
    const stats = fs.statSync(testFilePath);
    console.log('Test file stats:', {
      path: testFilePath,
      size: `${(stats.size / 1024).toFixed(2)} KB`,
      exists: fs.existsSync(testFilePath),
      lastModified: stats.mtime
    });

    // Validate file
    await transcriptionService.validateAudioFile(testFilePath);

    // Read first few bytes to verify it's a WebM file
    const buffer = fs.readFileSync(testFilePath, { length: 4 });
    console.log('File header:', buffer.toString('hex'));
    
    // Attempt transcription
    console.log('Attempting transcription...');
    const result = await transcriptionService.transcribeFile(testFilePath, 'test-session');

    console.log('Transcription successful!');
    console.log('Result:', result);

    return result;
  } catch (error) {
    console.error('Transcription test failed:', error);
    console.error('Full error:', JSON.stringify(error, null, 2));
    throw error;
  }
}

// Run the test
testTranscription()
  .then(() => {
    console.log('Test completed successfully');
    process.exit(0);
  })
  .catch((error) => {
    console.error('Test failed:', error);
    process.exit(1);
  });

================
File: src/lib/types/audio.ts
================
// src/lib/types/audio.ts

export interface AudioChunkMetadata {
  type: 'metadata';
  chunkId: number;
  timestamp: number;
  size: number;
  checksum: string;
  sessionId: string;
  [key: string]: unknown; // Allow additional fields
}

export type SessionStatus =
  | 'recording'
  | 'initializing'
  | 'processing'
  | 'transcribing'
  | 'completed'
  | 'stopped'
  | 'ready'
  | 'failed';

export interface SessionMetadata {
  sessionId: string; // Unique ID for the session
  startTime: number; // Timestamp when the session started
  status: SessionStatus; // Current session status
  chunks: AudioChunkMetadata[]; // List of audio chunks
  totalDuration: number; // Total duration of audio in milliseconds
  totalSize: number; // Total size of all chunks in bytes
  lastChunkId: number; // Last processed chunk ID
  checksums: string[]; // List of checksums for chunk validation
  transcription: TranscriptionData | null; // Final transcription (if available)
}

export interface TranscriptionData {
  text: string; // Transcribed text
  timestamp: number; // Timestamp when the transcription was created
  sessionId: string; // Associated session ID
}

export type WebSocketMessageType =
  | 'command'
  | 'chunk'
  | 'status'
  | 'error'
  | 'recording_complete'
  | 'ack'
  | 'transcription'
  | 'session_ended';

export interface WebSocketMessage {
  type: WebSocketMessageType; // Type of WebSocket message
  payload: WebSocketPayload; // Message payload
  sessionId: string; // Session ID associated with the message
  timestamp: number; // Timestamp of the message
}

export interface WebSocketPayload {
  message?: string; // Optional error or status message
  status?: SessionStatus; // Current session status
  sessionId?: string; // Session ID for context
  action?: 'start' | 'stop' | 'end' | 'startRecording'; // Action being requested
  chunkId?: number; // ID of the audio chunk
  duration?: number; // Duration of the audio chunk
  size?: number; // Size of the audio chunk in bytes
  transcription?: TranscriptionData; // Transcription data (if available)
  path?: string; // Path to a recording file
  [key: string]: unknown; // Allow additional fields with unknown type
}

// Function type for creating WebSocket messages
export type CreateWebSocketMessage = <T extends WebSocketMessageType>(
  type: T,
  payload: WebSocketPayload,
  sessionId: string
) => WebSocketMessage;

// Default session data to use as a fallback
export const defaultSessionData: SessionMetadata = {
  sessionId: '',
  startTime: Date.now(),
  status: 'initializing',
  chunks: [],
  totalDuration: 0,
  totalSize: 0,
  lastChunkId: -1,
  checksums: [],
  transcription: null,
};

// Hook return type for the audio recorder
export interface AudioRecorderHook {
  isRecording: boolean; // Is recording currently active
  error: string | null; // Current error message (if any)
  audioLevel: number; // Current audio level (for visualization)
  isConnected: boolean; // Is the WebSocket connection active
  sessionData: SessionMetadata | null; // Metadata for the current session
  transcriptions: TranscriptionData[]; // List of received transcriptions
  sessionActive: boolean; // Is the session currently active
  sessionId: string | null; // Current session ID
  startSession: (providedSessionId?: string) => Promise<string>; // Start a new session
  startRecording: () => Promise<void>; // Start recording
  stopRecording: () => void; // Stop recording
  endSession: () => void; // End the current session
  sendWebSocketMessage: (message: WebSocketMessage) => void; // Send a WebSocket message
}

================
File: src/lib/types/transcription.ts
================
export interface TranscriptionResponse {
    text: string;
    timestamp: number;
    sessionId: string;
  }
  
  export interface TranscriptionSegment {
    id: string;
    text: string;
    timestamp: number;
    isEditable?: boolean;
  }

================
File: src/lib/utils/audio.ts
================
// src/lib/utils/audio.ts
export async function calculateChecksum(data: ArrayBuffer): Promise<string> {
    const hashBuffer = await crypto.subtle.digest('SHA-256', data);
    const hashArray = Array.from(new Uint8Array(hashBuffer));
    return hashArray.map(b => b.toString(16).padStart(2, '0')).join('');
  }

================
File: src/lib/utils/env.ts
================
export const OPENAI_API_KEY = process.env.OPENAI_API_KEY || '';

================
File: src/lib/utils/session.js
================
// src/lib/utils/session.js
export function normalizeSessionId(sessionId) {
    if (!sessionId) return '';
    
    const parts = sessionId.split('-');
    
    if (parts.length >= 3 && parts[0] === 'session') {
      return `session-${parts[1]}`;
    }
    
    if (parts.length === 2 && parts[0] === 'session') {
      return sessionId;
    }
    
    return sessionId;
  }

================
File: src/lib/utils/session.ts
================
// src/lib/utils/session.ts
export function normalizeSessionId(sessionId: string): string {
    if (!sessionId) return '';
    
    // Split by hyphens
    const parts = sessionId.split('-');
    
    // Look for the timestamp part (13-digit number)
    for (let i = 0; i < parts.length; i++) {
      if (/^\d{13}$/.test(parts[i])) {
        return `session-${parts[i]}`;
      }
    }
    
    // If no timestamp found, return original
    return sessionId;
  }

================
File: src/lib/utils/ui.ts
================
// src/lib/utils/ui.ts
import { type ClassValue, clsx } from 'clsx';
import { twMerge } from 'tailwind-merge';

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs));
}

================
File: src/metadata/session-1731687523772-iqjwhjfia-transcription.json
================
{
  "transcriptions": [],
  "sessionId": "session-1731687523772-iqjwhjfia",
  "lastUpdated": 1731687535081
}

================
File: src/metadata/session-1731687523772-iqjwhjfia.json
================
{
  "totalDuration": 3000,
  "totalSize": 144017,
  "lastChunkId": 2,
  "checksums": {},
  "transcriptions": []
}

================
File: src/metadata/session-1731687589322-8r4i67ak1-transcription.json
================
{
  "transcriptions": [],
  "sessionId": "session-1731687589322-8r4i67ak1",
  "lastUpdated": 1731687616155
}

================
File: src/metadata/session-1731687589322-8r4i67ak1.json
================
{
  "totalDuration": 5000,
  "totalSize": 199835,
  "lastChunkId": 4,
  "checksums": {},
  "transcriptions": []
}

================
File: src/metadata/session-1731688034377-5wnbspbnh-transcription.json
================
{
  "sessionId": "session-1731688034377-5wnbspbnh",
  "lastUpdated": 1731688100068,
  "transcriptions": [
    {
      "text": "A police officer.",
      "timestamp": 1731688043235,
      "sessionId": "session-1731688034377-5wnbspbnh",
      "chunkId": 0
    },
    {
      "text": "A police officer who is half human.",
      "timestamp": 1731688090189,
      "sessionId": "session-1731688034377-5wnbspbnh",
      "chunkId": 4
    }
  ]
}

================
File: src/metadata/session-1731688034377-5wnbspbnh.json
================
{
  "totalDuration": 6000,
  "totalSize": 268725,
  "lastChunkId": 5,
  "checksums": {},
  "transcriptions": [
    {
      "text": "A police officer.",
      "timestamp": 1731688043235,
      "sessionId": "session-1731688034377-5wnbspbnh",
      "chunkId": 0
    },
    {
      "text": "A police officer who is half human.",
      "timestamp": 1731688090189,
      "sessionId": "session-1731688034377-5wnbspbnh",
      "chunkId": 4
    }
  ]
}

================
File: src/metadata/session-1731688189665-i25jbs76v-transcription.json
================
{
  "sessionId": "session-1731688189665-i25jbs76v",
  "lastUpdated": 1731688201916,
  "transcriptions": [
    {
      "text": "A monkey who was a.",
      "timestamp": 1731688195839,
      "sessionId": "session-1731688189665-i25jbs76v",
      "chunkId": 0
    }
  ]
}

================
File: src/metadata/session-1731688189665-i25jbs76v.json
================
{
  "totalDuration": 3000,
  "totalSize": 109135,
  "lastChunkId": 2,
  "checksums": {},
  "transcriptions": [
    {
      "text": "A monkey who was a.",
      "timestamp": 1731688195839,
      "sessionId": "session-1731688189665-i25jbs76v",
      "chunkId": 0
    }
  ]
}

================
File: src/metadata/session-1731688243886-grtu7fqfa-transcription.json
================
{
  "sessionId": "session-1731688243886-grtu7fqfa",
  "lastUpdated": 1731688408295,
  "transcriptions": [
    {
      "text": "A half-human, half-dog police officer.",
      "timestamp": 1731688248948,
      "sessionId": "session-1731688243886-grtu7fqfa",
      "chunkId": 0
    }
  ]
}

================
File: src/metadata/session-1731688243886-grtu7fqfa.json
================
{
  "totalDuration": 2000,
  "totalSize": 92772,
  "lastChunkId": 1,
  "checksums": {},
  "transcriptions": [
    {
      "text": "A half-human, half-dog police officer.",
      "timestamp": 1731688248948,
      "sessionId": "session-1731688243886-grtu7fqfa",
      "chunkId": 0
    }
  ]
}

================
File: src/metadata/session-1731714341469-bokt8mv1t-transcription.json
================
{
  "sessionId": "session-1731714341469-bokt8mv1t",
  "lastUpdated": 1731714396943,
  "transcriptions": [
    {
      "text": "This is recording number one.",
      "timestamp": 1731714353135,
      "sessionId": "session-1731714341469-bokt8mv1t",
      "chunkId": 0
    },
    {
      "text": "This is recording number four.",
      "timestamp": 1731714385720,
      "sessionId": "session-1731714341469-bokt8mv1t",
      "chunkId": 8
    }
  ]
}

================
File: src/metadata/session-1731714341469-bokt8mv1t.json
================
{
  "totalDuration": 10000,
  "totalSize": 418103,
  "lastChunkId": 9,
  "checksums": {},
  "transcriptions": [
    {
      "text": "This is recording number one.",
      "timestamp": 1731714353135,
      "sessionId": "session-1731714341469-bokt8mv1t",
      "chunkId": 0
    },
    {
      "text": "This is recording number four.",
      "timestamp": 1731714385720,
      "sessionId": "session-1731714341469-bokt8mv1t",
      "chunkId": 8
    }
  ]
}

================
File: src/metadata/session-1731715333724-qsuqfxz74-transcription.json
================
{
  "sessionId": "session-1731715333724-qsuqfxz74",
  "lastUpdated": 1731715669596,
  "transcriptions": [
    {
      "text": "This is recording number one.",
      "timestamp": 1731715340992,
      "sessionId": "session-1731715333724-qsuqfxz74",
      "chunkId": 0
    },
    {
      "text": "This is recording number five.",
      "timestamp": 1731715376486,
      "sessionId": "session-1731715333724-qsuqfxz74",
      "chunkId": 7
    }
  ]
}

================
File: src/metadata/session-1731715333724-qsuqfxz74.json
================
{
  "totalDuration": 30000,
  "totalSize": 724941,
  "lastChunkId": 9,
  "checksums": {},
  "transcriptions": [
    {
      "text": "This is recording number one.",
      "timestamp": 1731715340992,
      "sessionId": "session-1731715333724-qsuqfxz74",
      "chunkId": 0
    },
    {
      "text": "This is recording number five.",
      "timestamp": 1731715376486,
      "sessionId": "session-1731715333724-qsuqfxz74",
      "chunkId": 7
    }
  ]
}

================
File: src/metadata/session-1731715712023-4ndrqiuus-transcription.json
================
{
  "sessionId": "session-1731715712023-4ndrqiuus",
  "lastUpdated": 1731715736733,
  "transcriptions": [
    {
      "text": "This is recording one.",
      "timestamp": 1731715719145,
      "sessionId": "session-1731715712023-4ndrqiuus",
      "chunkId": 0
    }
  ]
}

================
File: src/metadata/session-1731715712023-4ndrqiuus.json
================
{
  "totalDuration": 12000,
  "totalSize": 322481,
  "lastChunkId": 3,
  "checksums": {},
  "transcriptions": [
    {
      "text": "This is recording one.",
      "timestamp": 1731715719145,
      "sessionId": "session-1731715712023-4ndrqiuus",
      "chunkId": 0
    }
  ]
}

================
File: src/metadata/session-1731756619590-g1jd0yko1-transcription.json
================
{
  "sessionId": "session-1731756619590-g1jd0yko1",
  "lastUpdated": 1731756620096,
  "transcriptions": []
}

================
File: src/metadata/session-1731756619590-g1jd0yko1.json
================
{
  "totalDuration": 18000,
  "totalSize": 462557,
  "lastChunkId": 5,
  "checksums": {},
  "transcriptions": []
}

================
File: src/metadata/session-1731796422917-7khc3ky8x-transcription.json
================
{
  "sessionId": "session-1731796422917-7khc3ky8x",
  "lastUpdated": 1731796423475,
  "transcriptions": []
}

================
File: src/metadata/session-1731796422917-7khc3ky8x.json
================
{
  "totalDuration": 3000,
  "totalSize": 161471,
  "lastChunkId": 0,
  "checksums": {},
  "transcriptions": []
}

================
File: src/metadata/session-1731796721371-19ifajlcu-transcription.json
================
{
  "sessionId": "session-1731796721371-19ifajlcu",
  "lastUpdated": 1731796799366,
  "transcriptions": [
    {
      "text": "This is the first recording and it should stop in just a few seconds.",
      "timestamp": 1731796736359,
      "sessionId": "session-1731796721371-19ifajlcu",
      "chunkId": 0
    }
  ]
}

================
File: src/metadata/session-1731796721371-19ifajlcu.json
================
{
  "totalDuration": 3000,
  "totalSize": 160418,
  "lastChunkId": 0,
  "checksums": {},
  "transcriptions": [
    {
      "text": "This is the first recording and it should stop in just a few seconds.",
      "timestamp": 1731796736359,
      "sessionId": "session-1731796721371-19ifajlcu",
      "chunkId": 0
    }
  ]
}

================
File: src/metadata/session-1731797118978-pg1npt5je-transcription.json
================
{
  "sessionId": "session-1731797118978-pg1npt5je",
  "lastUpdated": 1731797318760,
  "transcriptions": [
    {
      "text": "This is recording number one, and I'm expecting it to work.",
      "timestamp": 1731797137254,
      "sessionId": "session-1731797118978-pg1npt5je",
      "chunkId": 0
    }
  ]
}

================
File: src/metadata/session-1731797118978-pg1npt5je.json
================
{
  "totalDuration": 3000,
  "totalSize": 160382,
  "lastChunkId": 0,
  "checksums": {},
  "transcriptions": [
    {
      "text": "This is recording number one, and I'm expecting it to work.",
      "timestamp": 1731797137254,
      "sessionId": "session-1731797118978-pg1npt5je",
      "chunkId": 0
    }
  ]
}

================
File: src/metadata/session-1731797976298-r7czxv3ju-transcription.json
================
{
  "sessionId": "session-1731797976298-r7czxv3ju",
  "lastUpdated": 1731798220054,
  "transcriptions": []
}

================
File: src/metadata/session-1731797976298-r7czxv3ju.json
================
{
  "totalDuration": 0,
  "totalSize": 0,
  "lastChunkId": -1,
  "checksums": {},
  "transcriptions": []
}

================
File: src/metadata/session-1732057400077-zqu6yf5cn-transcription.json
================
{
  "sessionId": "session-1732057400077-zqu6yf5cn",
  "lastUpdated": 1732057420143,
  "transcriptions": []
}

================
File: src/metadata/session-1732057400077-zqu6yf5cn.json
================
{
  "totalDuration": 10000,
  "totalSize": 75651,
  "lastChunkId": 0,
  "checksums": {},
  "transcriptions": []
}

================
File: src/metadata/session-1732057719467-3stymg4-transcription.json
================
{
  "sessionId": "session-1732057719467-3stymg4",
  "lastUpdated": 1732057769785,
  "transcriptions": []
}

================
File: src/metadata/session-1732057719467-3stymg4.json
================
{
  "totalDuration": 0,
  "totalSize": 0,
  "lastChunkId": -1,
  "checksums": {},
  "transcriptions": []
}

================
File: src/metadata/session-1732057770977-udehg9u-transcription.json
================
{
  "sessionId": "session-1732057770977-udehg9u",
  "lastUpdated": 1732057775375,
  "transcriptions": []
}

================
File: src/metadata/session-1732057770977-udehg9u.json
================
{
  "totalDuration": 0,
  "totalSize": 0,
  "lastChunkId": -1,
  "checksums": {},
  "transcriptions": []
}

================
File: src/metadata/session-1732057776677-bfdsnja-transcription.json
================
{
  "sessionId": "session-1732057776677-bfdsnja",
  "lastUpdated": 1732057780247,
  "transcriptions": []
}

================
File: src/metadata/session-1732057776677-bfdsnja.json
================
{
  "totalDuration": 0,
  "totalSize": 0,
  "lastChunkId": -1,
  "checksums": {},
  "transcriptions": []
}

================
File: src/metadata/session-1732057849554-3m89hft-transcription.json
================
{
  "sessionId": "session-1732057849554-3m89hft",
  "lastUpdated": 1732058084777,
  "transcriptions": []
}

================
File: src/metadata/session-1732057849554-3m89hft.json
================
{
  "totalDuration": 20000,
  "totalSize": 143611,
  "lastChunkId": 1,
  "checksums": {},
  "transcriptions": []
}

================
File: src/metadata/session-1732058087531-b8i4voq-transcription.json
================
{
  "sessionId": "session-1732058087531-b8i4voq",
  "lastUpdated": 1732058849239,
  "transcriptions": []
}

================
File: src/metadata/session-1732058087531-b8i4voq.json
================
{
  "totalDuration": 40000,
  "totalSize": 293685,
  "lastChunkId": 3,
  "checksums": {},
  "transcriptions": []
}

================
File: src/metadata/session-1732059156755-xc2ez90-transcription.json
================
{
  "sessionId": "session-1732059156755-xc2ez90",
  "lastUpdated": 1732059766940,
  "transcriptions": []
}

================
File: src/metadata/session-1732059156755-xc2ez90.json
================
{
  "totalDuration": 50000,
  "totalSize": 359489,
  "lastChunkId": 4,
  "checksums": {},
  "transcriptions": []
}

================
File: src/metadata/session-1732061955759-anhn84i-transcription.json
================
{
  "sessionId": "session-1732061955759-anhn84i",
  "lastUpdated": 1732062462027,
  "transcriptions": []
}

================
File: src/metadata/session-1732061955759-anhn84i.json
================
{
  "totalDuration": 0,
  "totalSize": 0,
  "lastChunkId": -1,
  "checksums": {},
  "transcriptions": []
}

================
File: src/env.js
================
// src/env.js
export const OPENAI_API_KEY = process.env.OPENAI_API_KEY || '';

================
File: .eslintrc.json
================
{
  "extends": ["next/core-web-vitals", "next/typescript"]
}

================
File: .gitignore
================
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.*
.yarn/*
!.yarn/patches
!.yarn/plugins
!.yarn/releases
!.yarn/versions

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# env files (can opt-in for commiting if needed)
.env*

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts

#audio files
*.webm
/metadata

#image files
*.png

================
File: file-watcher.js
================
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import OpenAI from 'openai';
import { generateMemoryPrompt } from './src/lib/services/memoryPrompt.js';
import { generateImagePrompt } from './src/lib/services/generateImagePrompt.js';
import { generateImageFlux } from './src/lib/services/generateImageFlux.js';
import { normalizeSessionId } from './src/lib/utils/session.js';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

const openai = new OpenAI();

// File paths
const transcriptionDir = path.join(__dirname, 'metadata');
const memoryFilePath = path.join(transcriptionDir, 'memory-log.txt');
const imagesDirPath = path.join(__dirname, 'generated_images');

// Ensure required directories exist
if (!fs.existsSync(transcriptionDir)) {
  fs.mkdirSync(transcriptionDir, { recursive: true });
}
if (!fs.existsSync(imagesDirPath)) {
  fs.mkdirSync(imagesDirPath);
}

// Memory structure
let memory = {
  characters: [],
  items: [],
  locations: [],
};

// Track processed chunks by session
//const processedChunks = new Map(); // sessionId -> Set of processed chunkIds

// Add new reset function
export const resetMemory = () => {
  console.log('Resetting memory state...');
  memory = {
    characters: [],
    items: [],
    locations: [],
  };
  saveMemory(JSON.stringify(memory, null, 2));
  console.log('Memory reset completed');
};

// Load memory
const loadMemory = () => {
  try {
    if (fs.existsSync(memoryFilePath)) {
      const data = fs.readFileSync(memoryFilePath, 'utf-8');
      memory = JSON.parse(data);
      console.log('Memory loaded:', memory);
    } else {
      // Initialize empty memory file if it doesn't exist
      const initialMemory = {
        characters: [],
        items: [],
        locations: [],
      };
      saveMemory(JSON.stringify(initialMemory, null, 2));
      memory = initialMemory;
      console.log('Created new memory file with initial state');
    }
  } catch (err) {
    console.error('Error loading memory, starting fresh:', err);
    memory = {
      characters: [],
      items: [],
      locations: [],
    };
  }
};

// Updated saveMemory function (removed redundant directory check)
const saveMemory = (memoryUpdate) => {
  try {
    console.log('Saving memory to:', memoryFilePath);
    console.log('Memory content:', memoryUpdate);

    // If memoryUpdate is an object, stringify it
    const contentToWrite = typeof memoryUpdate === 'string' 
      ? memoryUpdate 
      : JSON.stringify(memoryUpdate, null, 2);

    fs.writeFileSync(memoryFilePath, contentToWrite);
    console.log('Memory file saved successfully at:', memoryFilePath);
  } catch (err) {
    console.error('Error saving memory:', err);
    console.error('Failed path:', memoryFilePath);
    console.error('Content type:', typeof memoryUpdate);
    console.error('Error details:', err.message);
  }
};

// Process new transcription with context
const processTranscriptionData = async (transcriptionData) => {
  const { transcriptions, sessionId } = transcriptionData;
  const normalizedSessionId = normalizeSessionId(sessionId);
  
  // Get the latest transcription
  const newTranscription = transcriptions[transcriptions.length - 1];

  if (!newTranscription) return; // Nothing new to process
  
  // Get recent context (last 3 transcriptions before the new one)
  const recentContext = transcriptions
    .slice(-4, -1) // Get up to 3 previous transcriptions
    .map(t => t.text)
    .join(' ');

  console.log('Processing new transcription:', {
    text: newTranscription.text,
    context: recentContext,
    sessionId: normalizedSessionId,
  });

  try {
    // 1. Generate image first (for lower latency)
    console.log('Generating image prompt...');
    const imagePrompt = await generateImagePrompt(
      newTranscription.text, 
      recentContext,
      memory
    );
    if (imagePrompt) {
      console.log('Image prompt received:', imagePrompt);
      const imagePath = await generateImageFlux(imagePrompt, normalizedSessionId);
      if (imagePath) {
        console.log(`Image generated and saved at: ${imagePath}`);
      }
    }

    // 2. Update memory after image generation
    const prompt = generateMemoryPrompt(newTranscription.text, memory, recentContext);
    console.log('Calling LLM for memory updates...');
    const response = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: prompt }],
    });

    const updatedMemory = response.choices[0]?.message?.content;
    if (updatedMemory) {
      console.log('Received memory update:', updatedMemory);
      try {
        memory = JSON.parse(updatedMemory);
        saveMemory(JSON.stringify(memory, null, 2));
        console.log('Memory state updated:', memory);
      } catch (err) {
        console.error('Error parsing memory update:', err);
      }
    }
  } catch (error) {
    console.error('Error processing transcription:', error);
  }
};

const startFileWatcher = () => {
  console.log('Starting file watcher...');
  loadMemory();

  console.log('Watching directory for transcription changes:', transcriptionDir);
  fs.watch(transcriptionDir, (eventType, filename) => {
    if (filename && filename.endsWith('-transcription.json')) {
      const filePath = path.join(transcriptionDir, filename);

      fs.readFile(filePath, 'utf-8', async (err, data) => {
        if (err) {
          console.error(`Error reading transcription file ${filename}:`, err);
          return;
        }

        try {
          const transcriptionData = JSON.parse(data);
          if (transcriptionData.transcriptions?.length > 0) {
            await processTranscriptionData(transcriptionData);
          }
        } catch (err) {
          console.error('Error processing transcription file:', err);
        }
      });
    }
  });
};

export const fileWatcher = {
  start: startFileWatcher,
  resetMemory
};

================
File: llmtest.js
================
import OpenAI from "openai"; // Ensure you're using the latest OpenAI SDK
import dotenv from "dotenv"; // To load environment variables

dotenv.config(); // Load variables from a .env file if needed

// Initialize OpenAI client
const openai = new OpenAI(); // Automatically uses the OPENAI_API_KEY

const testLLMCall = async () => {
  try {
    console.log("Testing LLM call...");
    const response = await openai.chat.completions.create({
      model: "gpt-4",
      messages: [
        { role: "system", content: "You are a helpful assistant." },
        { role: "user", content: "Write a haiku about autumn." },
      ],
    });

    console.log("LLM Response:", response.choices[0].message.content);
  } catch (error) {
    console.error("Error during LLM call:", error);
  }
};

testLLMCall();

================
File: next.config.ts
================
import type { NextConfig } from 'next';

const config: NextConfig = {
  webpack: (config) => {
    config.externals.push({
      'utf-8-validate': 'commonjs utf-8-validate',
      'bufferutil': 'commonjs bufferutil',
    });
    return config;
  },
  images: {
    remotePatterns: [
      {
        protocol: 'http',
        hostname: 'localhost',
        port: '3001',
        pathname: '/api/images/**',
      },
    ],
  },
  async rewrites() {
    return [
      {
        source: '/api/socket',
        destination: '/api/socket/route',
      },
      {
        source: '/socket.io/:path*',
        destination: '/api/socket/:path*',
      },
    ];
  },
};

export default config;

================
File: package.json
================
{
  "name": "mythra-web",
  "version": "0.1.0",
  "type": "module",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "@ffmpeg-installer/ffmpeg": "^1.1.0",
    "@radix-ui/react-icons": "^1.3.1",
    "@types/express": "^5.0.0",
    "@types/ws": "^8.5.13",
    "axios": "^1.7.7",
    "class-variance-authority": "^0.7.0",
    "clsx": "^2.1.1",
    "dotenv": "^16.4.5",
    "express": "^4.21.1",
    "fluent-ffmpeg": "^2.1.3",
    "form-data": "^4.0.1",
    "lucide-react": "^0.454.0",
    "next": "15.0.2",
    "node-fetch": "^2.7.0",
    "openai": "^4.71.1",
    "react": "19.0.0-rc-02c0e824-20241028",
    "react-dom": "19.0.0-rc-02c0e824-20241028",
    "shadcn-ui": "^0.9.2",
    "socket.io": "^4.8.1",
    "socket.io-client": "^4.8.1",
    "tailwind-merge": "^2.5.4",
    "ts-node": "^10.9.2",
    "ws": "^8.18.0"
  },
  "devDependencies": {
    "@types/node": "^20",
    "@types/react": "^18",
    "@types/react-dom": "^18",
    "eslint": "^8",
    "eslint-config-next": "15.0.2",
    "postcss": "^8",
    "tailwindcss": "^3.4.1",
    "typescript": "^5.6.3"
  }
}

================
File: postcss.config.mjs
================
/** @type {import('postcss-load-config').Config} */
const config = {
  plugins: {
    tailwindcss: {},
  },
};

export default config;

================
File: printservice.py
================
import subprocess
import time
import os
import sys

def push_image_to_phone(image_name):
    """
    Pushes the specified image to the phone's Pictures folder and refreshes the media database.
    """
    source_path = os.path.join("generated_images", image_name)
    target_directory = "/storage/emulated/0/Pictures/"
    
    # Step 1: Push the image to the phone
    print(f"Pushing image {image_name} to the phone...")
    subprocess.run(["adb", "push", source_path, target_directory], check=True)
    print(f"Image {image_name} pushed successfully to {target_directory}")

    # Step 2: Refresh media database
    print("Refreshing media database...")
    subprocess.run(["adb", "shell", "am", "broadcast", "-a", "android.intent.action.MEDIA_SCANNER_SCAN_FILE", "-d", f"file://{target_directory}{image_name}"], check=True)
    print("Media database refreshed successfully")

def kill_all_apps():
    print("Killing all background apps...")
    command = ["adb", "shell", "am", "kill-all"]
    subprocess.run(command, check=True)
    print("All background apps killed successfully")

def go_to_home_screen():
    print("Returning to home screen...")
    command = ["adb", "shell", "input", "keyevent", "KEYCODE_HOME"]
    subprocess.run(command, check=True)
    print("Returned to home screen successfully")

def simulate_tap(x, y):
    print(f"Simulating tap on ({x}, {y})")  # Debugging output
    command = ["adb", "shell", "input", "tap", str(x), str(y)]
    subprocess.run(command, check=True)
    print(f"Tapped on ({x}, {y}) successfully")

def automate_printing_workflow(image_name):
    """
    Orchestrates the entire workflow: pushes image, prepares phone, and triggers printing.
    """
    # Step 1: Push the image to the phone and refresh media
    push_image_to_phone(image_name)

    # Step 2: Kill all background apps
    kill_all_apps()
    time.sleep(1)

    # Step 3: Ensure we're on the home screen
    go_to_home_screen()
    time.sleep(1)

    # Step 4: Open the HP Sprocket Panorama app
    print("Opening HP Sprocket Panorama app...")
    subprocess.run(["adb", "shell", "am", "start", "-n", "com.hp.impulse.panorama/.activity.SplashActivity"], check=True)
    print("Opened HP Sprocket app")

    # Step 5: Wait for the app to load
    time.sleep(3)

    # Step 6: Simulate taps
    simulate_tap(558, 1911)  # Print Photo button (quick print)
    time.sleep(2)
    simulate_tap(160, 480)  # Select image
    time.sleep(2)
    simulate_tap(995, 240)  # Hit the next button
    time.sleep(3)
    simulate_tap(1000, 226)  # Hit the print icon
    time.sleep(2)
    simulate_tap(550, 2140)  # Hit the print button (bottom middle)
    print("Automated printing workflow completed")

    # Step 7: Wait 40 seconds for the print job to process
    print("Waiting 40 seconds for the print job...")
    time.sleep(40)

    # Step 8: Kill the HP Sprocket Panorama app
    print("Killing HP Sprocket app...")
    subprocess.run(["adb", "shell", "am", "force-stop", "com.hp.impulse.panorama"], check=True)
    print("HP Sprocket app killed. Ready for the next session.")

# Main entry point
if __name__ == "__main__":
    # Ensure the script receives an image name as a command-line argument
    if len(sys.argv) < 2:
        print("Usage: python3 printservice.py <image_name>")
        sys.exit(1)
    
    # Get the image name from the command-line arguments
    image_name = sys.argv[1]
    print(f"Image name received: {image_name}")

    # Run the printing workflow
    automate_printing_workflow(image_name)

================
File: README.md
================
This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://nextjs.org/docs/app/api-reference/cli/create-next-app).

## Getting Started

First, run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.

This project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!

## Deploy on Vercel

The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.

Check out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.

================
File: server.mjs
================
// server.mjs
import { WebSocketServer } from 'ws';
import { fileURLToPath } from 'url';
import { dirname, join } from 'path';
import { mkdirSync } from 'fs';
import fs from 'fs/promises';
import express from 'express';
import cors from 'cors';
import { fileWatcher } from './file-watcher.js';
import { normalizeSessionId } from './src/lib/utils/session.js';
import { AudioSession } from './src/lib/services/audioSession.js';
import TranscriptionService from './src/lib/services/transcription.js';

// Get the current directory
const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

// Constants and configurations
const AUDIO_DIR = join(__dirname, 'audio-chunks');
const RECORDINGS_DIR = join(__dirname, 'recordings');
const METADATA_DIR = join(__dirname, 'metadata');
const IMAGES_DIR = join(__dirname, 'generated_images');

const cleanupSession = async (session) => {
  if (!session) return;
  
  try {
    console.log('Server: Starting cleanup of session:', session.sessionId);
    // Stop recording if active
    if (session.recordingService.isRecording(session.sessionId)) {
      await session.stopRecording();
    }
    
    // Finalize session
    await session.finalize();
    
    // Clear from active sessions
    const removed = activeSessions.delete(session.sessionId);
    console.log('Server: Session removed from active sessions:', removed);
    console.log('Server: Remaining active sessions:', activeSessions.size);
  } catch (error) {
    console.error('Error cleaning up session:', error);
  }
};

console.log('=== Initializing Server ===');

// Ensure all required directories exist
try {
  for (const dir of [AUDIO_DIR, RECORDINGS_DIR, METADATA_DIR, IMAGES_DIR]) {
    mkdirSync(dir, { recursive: true });
    console.log(`Directory created/verified: ${dir}`);
  }
} catch (error) {
  console.error('Failed to create directories:', error);
  process.exit(1);
}

// Session tracking
const activeSessions = new Map();

const server = new WebSocketServer({ port: 8080 });
console.log('WebSocket server running on ws://localhost:8080');

server.on('connection', (socket) => {
  console.log('=== New Client Connected ===');
  let currentSession = null;
  const transcriptionService = new TranscriptionService();

  socket.on('message', async (data) => {
    try {
      const message = JSON.parse(data.toString());
      console.log('Server: Received message:', message);
  
      switch (message.type) {
        case 'session_reset':
          console.log('Server: Processing session reset command...');
          fileWatcher.resetMemory();
          socket.send(JSON.stringify({
            type: 'status',
            payload: { status: 'reset_complete' },
            sessionId: message.sessionId || '',
            timestamp: Date.now()
          }));
          break;
        case 'command':
  if (message.payload.action === 'start') {
    console.log('Server: Processing start command...');
    // Clean up existing session if there is one
    if (currentSession) {
      console.log('Server: Cleaning up existing session:', currentSession.sessionId);
      await cleanupSession(currentSession);
      currentSession = null;
    }

    const sessionId = message.payload.sessionId || `session-${Date.now()}`;
    console.log('Server: Starting new session:', sessionId);

    // Create session directory
    const sessionDir = join(AUDIO_DIR, sessionId);
    await fs.mkdir(sessionDir, { recursive: true });

    // Initialize new session and add to active sessions
    currentSession = new AudioSession(sessionId, transcriptionService);
    currentSession.setSocket(socket);
    activeSessions.set(sessionId, currentSession);
    console.log('Server: Session added to active sessions. Current count:', activeSessions.size);
    console.log('Server: Active sessions:', Array.from(activeSessions.keys()));

    // Send initial status
    socket.send(
      JSON.stringify({
        type: 'status',
        payload: { status: 'ready', sessionId },
        sessionId,
        timestamp: Date.now(),
      })
    );
  } else if (message.payload.action === 'startRecording') {
    console.log('Server: Processing startRecording command...');
    if (!currentSession) {
      console.error('Server: No active session found');
      // Try to recover by finding session in activeSessions map
      const sessionId = message.payload.sessionId;
      currentSession = activeSessions.get(sessionId);
      if (!currentSession) {
        console.error('Server: Could not recover session');
        return;
      }
      console.log('Server: Recovered session:', sessionId);
    }

    try {
      await currentSession.startRecording();
      currentSession.status = 'recording';
      console.log(`Server: Recording started for session ${currentSession.sessionId}`);

      socket.send(
        JSON.stringify({
          type: 'status',
          payload: { status: 'recording', sessionId: currentSession.sessionId },
          sessionId: currentSession.sessionId,
          timestamp: Date.now(),
        })
      );
    } catch (error) {
      console.error('Server: Error starting recording:', error);
    }
  } else if (message.payload.action === 'stop') {
    console.log('Server: Processing stop command with full message:', message);
    if (!currentSession) {
      console.error('Server: No active session found');
      // Try to recover session
      const sessionId = message.payload.sessionId;
      currentSession = activeSessions.get(sessionId);
      if (!currentSession) {
        console.error('Server: Could not recover session');
        return;
      }
      console.log('Server: Recovered session:', sessionId);
    }

    try {
      await currentSession.stopRecording();
      console.log('Server: Recording stopped, sending status update');

      // Ensure we update the session state
      currentSession.status = 'stopped';

      socket.send(
        JSON.stringify({
          type: 'status',
          payload: {
            status: 'stopped',
            sessionId: currentSession.sessionId,
          },
          sessionId: currentSession.sessionId,
          timestamp: Date.now(),
        })
      );
    } catch (error) {
      console.error('Server: Error during stop:', error);
    }
  } else if (message.payload.action === 'end') {
    console.log('Server: Processing end command...');
    if (!currentSession) {
      console.error('Server: No active session to end');
      throw new Error('No active session to end');
    }

    await cleanupSession(currentSession);
    currentSession = null;

    socket.send(
      JSON.stringify({
        type: 'session_ended',
        sessionId: message.payload.sessionId,
        timestamp: Date.now(),
      })
    );
  }
  break;
        default:
          console.warn('Unknown message type:', message.type);
          break;
      }
    } catch (error) {
      console.error('Error processing message:', error);
      socket.send(
        JSON.stringify({
          type: 'error',
          payload: { message: error.message },
          timestamp: Date.now(),
        })
      );
    }
  });
  

  // Update the close handler
  socket.on('close', async () => {
    console.log('Client disconnected, but keeping session active');
    // Remove cleanup code to keep session alive
  });

  socket.on('error', (error) => {
    console.error('WebSocket error:', error);
    socket.send(JSON.stringify({
      type: 'error',
      payload: { message: 'Internal server error' },
      timestamp: Date.now()
    }));
  });
});

// Clean up old sessions periodically
setInterval(() => {
  const now = Date.now();
  const maxAge = 24 * 60 * 60 * 1000; // 24 hours
  
  activeSessions.forEach((session, sessionId) => {
    if (now - session.startTime > maxAge) {
      activeSessions.delete(sessionId);
    }
  });
}, 60 * 60 * 1000); // Check every hour

// Log active connections every 30 seconds
setInterval(() => {
  console.log(`Active connections: ${server.clients.size}`);
  console.log(`Active sessions: ${activeSessions.size}`);
}, 30000);

const app = express();

// Move middleware setup before routes
app.use(cors());
app.use('/api/images', express.static(IMAGES_DIR));

app.get('/api/images/latest/:sessionId', async (req, res) => {
  try {
    const { sessionId } = req.params;
    console.log('Image request:', {
      requestedId: sessionId,
      normalizedId: normalizeSessionId(sessionId)
    });
    
    const files = await fs.readdir(IMAGES_DIR);
    const normalizedRequestId = normalizeSessionId(sessionId);
    
    const sessionImages = files.filter(file => {
      const normalizedFileId = normalizeSessionId(file.split('-')[0] + '-' + file.split('-')[1]);
      return normalizedFileId === normalizedRequestId;
    });

    console.log('Image matching:', {
      normalizedRequestId,
      foundImages: sessionImages
    });

    if (sessionImages.length > 0) {
      // Sort by timestamp to get the latest
      const latestImage = sessionImages.sort().reverse()[0];
      res.json({ imagePath: `/api/images/${latestImage}` });
    } else {
      res.status(404).json({ error: 'No images found for this session' });
    }
  } catch (error) {
    console.error('Error in image endpoint:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
});

// Start Express server
const PORT = 3001;
app.listen(PORT, () => {
  console.log(`Express API running on http://localhost:${PORT}`);
  // Start the file watcher after server is ready
  fileWatcher.start();
});

================
File: tailwind.config.ts
================
import type { Config } from 'tailwindcss'

const config: Config = {
  content: [
    './src/pages/**/*.{js,ts,jsx,tsx,mdx}',
    './src/components/**/*.{js,ts,jsx,tsx,mdx}',
    './src/app/**/*.{js,ts,jsx,tsx,mdx}',
  ],
  theme: {
    extend: {
      fontFamily: {
        obra: ['var(--font-obra)'],
        geist: ['var(--font-geist)'],
        'geist-mono': ['var(--font-geist-mono)'],
      },
    },
  },
  plugins: [],
}

export default config

================
File: test-file-watcher.js
================
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

// Resolve __dirname for ES modules
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Transcription file path (Simulated for testing)
const sessionId = 'session-test';
const transcriptionFilePath = path.join(__dirname, 'metadata', `${sessionId}-transcription.json`);

// Create or update the transcription file with a chunk
const createTranscriptionFile = (content) => {
    try {
      fs.writeFileSync(
        transcriptionFilePath,
        JSON.stringify({ text: content }, null, 2) // Match expected JSON structure
      );
      console.log(`Transcription file updated with content: "${content}"`);
    } catch (err) {
      console.error('Error creating transcription file:', err);
    }
  };

// Simulate feeding transcriptions into the system
const simulateTranscriptions = async () => {
  console.log('Starting transcription simulation...');
  const transcriptions = [
    { chunkId: 1, text: 'Bruce, the Goliath warrior, picks up a glowing sword.' },
    { chunkId: 2, text: 'Todd, the radiant Asmere Paladin, studies an ancient map.' },
    { chunkId: 3, text: 'The companions arrive at a mysterious cave covered in moss.' },
    { chunkId: 4, text: 'A goblin ambushes them as they enter the dark cave.' },
  ];

  let currentTranscriptions = [];

  for (let i = 0; i < transcriptions.length; i++) {
    currentTranscriptions.push(transcriptions[i]);
    console.log(`Feeding transcription ${i + 1}: ${transcriptions[i].text}`);
    createTranscriptionFile(currentTranscriptions);

    // Wait for the file watcher to process the transcription
    await new Promise((resolve) => setTimeout(resolve, 10000)); // 10-second delay
  }

  console.log('Finished transcription simulation.');
};

simulateTranscriptions();

================
File: test-generateimageprompt.js
================
import { generateImagePrompt } from './src/lib/services/generateImagePrompt.js';

const memory = {
  characters: [{ name: "Todd", description: "A radiant Asmere Paladin" }],
  items: [{ name: "Ancient Map", description: "A weathered map being studied by Todd" }],
  locations: [{ name: "Mysterious Cave", description: "A moss-covered cave" }],
};

const newText = "Todd enters the cave, his sword glowing faintly.";
const recentActivity = "Todd was studying an ancient map.";

console.log(generateImagePrompt(newText, recentActivity, memory));

================
File: test-metadata-permissions.js
================
import fs from 'fs/promises';
import path from 'path';

(async () => {
  const metadataPath = path.join(process.cwd(), 'metadata');
  const memoryLogPath = path.join(metadataPath, 'memory-log.txt');

  try {
    // Check directory access
    await fs.access(metadataPath);
    console.log(' Metadata directory is accessible.');

    // Check file read access
    const content = await fs.readFile(memoryLogPath, 'utf-8');
    console.log(' memory-log.txt is readable. Content:', content);

    // Check file write access
    await fs.writeFile(memoryLogPath, content + '\nTest Write Access');
    console.log(' memory-log.txt is writable. Test write succeeded.');

    // Cleanup test write
    const updatedContent = content.trim(); // Remove the test line
    await fs.writeFile(memoryLogPath, updatedContent);
    console.log(' Cleanup successful. File content restored.');
  } catch (error) {
    console.error(' Permission issue detected:', error);
  }
})();

================
File: test-transcription.mjs
================
import fs from 'fs/promises';
import path from 'path';
import TranscriptionService from './src/lib/services/transcription.js';

const TEST_DIRECTORY = '/Users/ethangrabau/Documents/mythra-web/audio-chunks/session-1732151066544-f60ydu6'; // Replace with the actual session ID

(async () => {
  try {
    // Initialize transcription service
    const transcriptionService = new TranscriptionService(); // Ensure this matches your service initialization

    // Read all files in the test directory
    const files = await fs.readdir(TEST_DIRECTORY);

    // Filter for .webm files
    const audioFiles = files.filter(file => file.endsWith('.webm'));

    console.log(`Found ${audioFiles.length} audio files for testing`);

    for (const file of audioFiles) {
      const filePath = path.join(TEST_DIRECTORY, file);

      console.log(`Testing transcription for file: ${filePath}`);

      try {
        // Run transcription
        const transcription = await transcriptionService.transcribeFile(filePath);

        console.log(`Transcription result for ${file}:`, transcription);
      } catch (error) {
        console.error(`Error transcribing file ${file}:`, error.message);
      }
    }
  } catch (err) {
    console.error('Error during testing:', err);
  }
})();

================
File: testRecording.js
================
import fs from 'fs';
import ffmpeg from 'fluent-ffmpeg';
import ffmpegInstaller from '@ffmpeg-installer/ffmpeg';

// Set ffmpeg path
ffmpeg.setFfmpegPath(ffmpegInstaller.path);

// Output directory for audio chunks
const outputDir = './test-chunks';
if (!fs.existsSync(outputDir)) {
  fs.mkdirSync(outputDir);
}

let chunkIndex = 0;
let recordingProcess = null;

// Function to start recording
const startRecording = () => {
  console.log('Starting recording...');

  const recordChunk = () => {
    const filename = `${outputDir}/chunk-${chunkIndex++}.webm`;

    recordingProcess = ffmpeg()
      .input(':0') // Use MacBook Pro Microphone (audio device [0])
      .inputFormat('avfoundation') // Specify AVFoundation for macOS
      .audioCodec('libopus') // Use opus codec for better compatibility with webm
      .format('webm') // Output format
      .duration(10) // Record for 10 seconds per chunk
      .on('start', (cmd) => {
        console.log(`Recording chunk ${chunkIndex} started with command: ${cmd}`);
      })
      .on('end', () => {
        console.log(`Chunk ${chunkIndex - 1} recording completed.`);
        if (chunkIndex < 3) {
          recordChunk(); // Start the next chunk if under 3 chunks
        } else {
          console.log('Recording completed after 3 chunks.');
        }
      })
      .on('error', (err) => {
        console.error(`Error during recording chunk ${chunkIndex - 1}:`, err.message);
      })
      .save(filename); // Save the chunk to a file
  };

  // Start recording the first chunk
  recordChunk();

  // Stop entire recording process after 30 seconds
  setTimeout(() => {
    if (recordingProcess) {
      recordingProcess.kill('SIGINT');
      console.log('Stopped recording after 30 seconds.');
    }
  }, 30000);
};

// Run the recording function
startRecording();

================
File: testWebSocketClient.js
================
import WebSocket from 'ws';

const socket = new WebSocket('ws://localhost:8080');

socket.on('open', () => {
  console.log('Connected to WebSocket server');
  socket.send('Hello Server!');
});

socket.on('message', (data) => {
  console.log('Received:', data);
});

socket.on('close', () => {
  console.log('Disconnected from WebSocket server');
});

socket.on('error', (error) => {
  console.error('WebSocket error:', error);
});

================
File: tree_exporter.py
================
import json

def print_tree(data, indent=0):
    for key, value in data.items():
        print(' ' * indent + str(key))
        if isinstance(value, dict):
            print_tree(value, indent + 4)
        elif isinstance(value, list):
            for item in value:
                if isinstance(item, dict):
                    print_tree(item, indent + 4)
                else:
                    print(' ' * (indent + 4) + str(item))
        else:
            print(' ' * (indent + 4) + str(value))

def main():
    with open('session-1730890313262.json', 'r') as file:
        data = json.load(file)
        print_tree(data)

if __name__ == "__main__":
    main()

================
File: tsconfig.json
================
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./src/*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}
